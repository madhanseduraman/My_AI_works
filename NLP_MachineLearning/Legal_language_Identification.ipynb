{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt   \n",
    "import matplotlib.style\n",
    "plt.style.use('classic')\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "#importing seaborn for statistical plots\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_data = pd.read_excel(path+'\\legal_data_1.0.xlsx',sheet_name='ResultLegal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Filename</th>\n",
       "      <th>FileContent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>Creating trust in \\r\\nthe digital world\\r\\nEYâ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>It \\r\\ncaptures the responses of 1,755 partici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>The following findings from the 36 participant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>But they also indicate the need for considerab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>Operating in a digital world invites  \\r\\nnew ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                           Filename  \\\n",
       "0      0  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "1      1  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "2      2  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "3      3  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "4      4  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "\n",
       "                                         FileContent  Label  \n",
       "0  Creating trust in \\r\\nthe digital world\\r\\nEYâ...      0  \n",
       "1  It \\r\\ncaptures the responses of 1,755 partici...      0  \n",
       "2  The following findings from the 36 participant...      0  \n",
       "3  But they also indicate the need for considerab...      0  \n",
       "4  Operating in a digital world invites  \\r\\nnew ...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGL_words = pd.read_excel(path+'\\legal_data_1.0.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Filename</th>\n",
       "      <th>FileContent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>Creating trust in \\r\\nthe digital world\\r\\nEYâ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>It \\r\\ncaptures the responses of 1,755 partici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>The following findings from the 36 participant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>But they also indicate the need for considerab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1003143 D4183 GISS 2 pager for Life Sciences_V...</td>\n",
       "      <td>Operating in a digital world invites  \\r\\nnew ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                           Filename  \\\n",
       "0      0  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "1      1  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "2      2  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "3      3  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "4      4  1003143 D4183 GISS 2 pager for Life Sciences_V...   \n",
       "\n",
       "                                         FileContent  Label  \n",
       "0  Creating trust in \\r\\nthe digital world\\r\\nEYâ...      0  \n",
       "1  It \\r\\ncaptures the responses of 1,755 partici...      0  \n",
       "2  The following findings from the 36 participant...      0  \n",
       "3  But they also indicate the need for considerab...      0  \n",
       "4  Operating in a digital world invites  \\r\\nnew ...      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#legal_data.legal_word.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30046\n",
       "1      755\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = legal_data.FileContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trust in \n",
      "the digital world\n",
      "EYâ€™s Global Information Security \n",
      "Survey 2015\n",
      "\n",
      "Life sciences sector results\n",
      "\n",
      "EYâ€™s Global Information Security Survey investigates the most important cybersecurity issues facing businesses today.\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = legal_data.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    #print(tag)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    output = tag_dict.get(tag[0]) \n",
    "    if output:\n",
    "        return output\n",
    "    else:\n",
    "        return 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( SD ):\n",
    "    lm = WordNetLemmatizer()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", SD)\n",
    "    #print(letters_only)\n",
    "    sentences = nltk.word_tokenize(str(letters_only))\n",
    "    #print(sentences)\n",
    "    sentences = [word.lower() for word in sentences]\n",
    "    no_stops = [word for word in sentences if not word in default_stopwords]\n",
    "    #print(no_stops)\n",
    "    lem_word = [lm.lemmatize(word,get_wordnet_pos(word)) for word in no_stops]\n",
    "    #print(lem_word)\n",
    "    #tags = st.tag(lem_word)\n",
    "    #tagged_words = [word for word,tag in tags if tag!='PERSON' and tag!='LOCATION']\n",
    "    tokens = [word.lower() for word in lem_word if len(word) > 2]\n",
    "    #print(tokens)\n",
    "    return( \" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It \\r\\ncaptures the responses of 1,755 participants around the globe and across industries, and we base our findings and conclusions \\r\\non those insights and on our extensive global experience working with clients to help improve their cybersecurity.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'capture response participant around globe across industry base finding conclusion insight extensive global experience work client help improve cybersecurity'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(X.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Parsed......................\n",
      "\n",
      "Wall time: 0 ns\n",
      "Cleaned Short Descriptions 5000 of 30801\n",
      "Cleaned Short Descriptions 10000 of 30801\n",
      "Cleaned Short Descriptions 15000 of 30801\n",
      "Cleaned Short Descriptions 20000 of 30801\n",
      "Cleaned Short Descriptions 25000 of 30801\n",
      "Cleaned Short Descriptions 30000 of 30801\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows Parsed......................\\n\")\n",
    "corpus = []\n",
    "num_SD = len(X)\n",
    "%time\n",
    "for i in range( 0, num_SD ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Cleaned Short Descriptions %d of %d\" % ( i+1, num_SD ))\n",
    "    corpus.append( review_to_words(str(X.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(corpus,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    operating digital world invite new challenge t...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doubt tax certainty project well work action discuss would useful regard also'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.text[30800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['leg'] = ''\n",
    "leg = []\n",
    "for idx, row in X.iterrows():\n",
    "    line_txt = X.text[idx]\n",
    "    for row1 in LGL_words.LGL:\n",
    "        if row1 in line_txt:\n",
    "            X['leg'][idx] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>leg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create trust digital world global information ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capture response participant around globe acro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>follow finding participant life science sector...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>also indicate need considerable improvement wo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating digital world invite new challenge t...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  leg\n",
       "0  create trust digital world global information ...     \n",
       "1  capture response participant around globe acro...     \n",
       "2  follow finding participant life science sector...     \n",
       "3  also indicate need considerable improvement wo...     \n",
       "4  operating digital world invite new challenge t...  yes"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.leg.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       25679\n",
       "yes     5122\n",
       "Name: leg, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.leg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.leg.replace(['','yes'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: leg, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.leg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30046\n",
       "1      755\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_test_txt, y_train, y_test,leg_train,leg_test = train_test_split(X.text, y,X.leg,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24640,)\n",
      "(6161,)\n",
      "(24640,)\n",
      "(6161,)\n",
      "(24640,)\n",
      "(6161,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_txt.shape)\n",
    "print(X_test_txt.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(leg_train.shape)\n",
    "print(leg_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(stop_words='english', ngram_range=(1,4),max_features=3000,lowercase=True)\n",
    "#X_dtm_train = tfidf_vec.fit_transform((X_train_txt))\n",
    "#X_dtm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dtm_test = tfidf_vec.transform(X_test_txt)\n",
    "#X_dtm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.DataFrame(corpus,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create trust digital world global information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capture response participant around globe acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>follow finding participant life science sector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>also indicate need considerable improvement wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>operating digital world invite new challenge t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  create trust digital world global information ...\n",
       "1  capture response participant around globe acro...\n",
       "2  follow finding participant life science sector...\n",
       "3  also indicate need considerable improvement wo...\n",
       "4  operating digital world invite new challenge t..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.concat([df_corpus,legal_data['Label'],X['leg']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('training_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset1 = pd.read_csv(path+'\\\\training_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29651\n",
       "1      755\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset1.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res = trainingset1.text\n",
    "y_res = trainingset1.Label\n",
    "feat_res = trainingset1.leg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_test_txt, y_train, y_test,feat_train,feat_test = train_test_split(trainingset1.text,trainingset1.Label,\n",
    "                                                                                   trainingset1.leg,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm = tfidf_vector.fit_transform(X_train_txt.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = np.array(feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = X_train_dtm.todense()\n",
    "feat_train = feat_train.reshape(-1,1)\n",
    "final_train_dtm = np.append(dense,feat_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24324,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_dtm.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ADASYN(random_state=11,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res_train_new, y_res_train_new = ros.fit_resample(final_train_dtm, y_train)\n",
    "#X_res_new, y_res_new = sm.fit_resample(final_train_dtm, y_res)\n",
    "#X_res_new, y_res_new = ada.fit_resample(final_train_dtm, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48058, 5001)\n",
      "(48058,)\n"
     ]
    }
   ],
   "source": [
    "print(X_res_train_new.shape)\n",
    "print(y_res_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm = tfidf_vector.transform(X_test_txt.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test = np.array(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = X_test_dtm.todense()\n",
    "feat_test = feat_test.reshape(-1,1)\n",
    "final_test_dtm = np.append(dense,feat_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res_test_new, y_res_test_new = ros.fit_resample(final_test_dtm, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12034, 5001)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12034,)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifer = RandomForestClassifier(n_estimators=200,criterion='entropy',random_state=0,class_weight={0:.3,1:.7},max_depth=200)\n",
    "rf_classifer.fit(X_res_train_new ,y_res_train_new)\n",
    "y_pred_randforest = rf_classifer.predict(X_res_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandForest ACCURACY: 0.5461193285690543\n",
      "RandForest CLASSIFIER CONFUSTION MATRIX: col_0     0    1\n",
      "row_0           \n",
      "0      5940   77\n",
      "1      5385  632\n",
      "===> **RandForest F1 SCORE [0.68504209 0.18792745]\n",
      "===> **RandForest Recall SCORE [0.98720293 0.10503573]\n",
      "===> **RandForest Precision SCORE [0.52450331 0.89139633]\n",
      "===> **RandForest ROC_AUC SCORE 0.5461193285690544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"RandForest ACCURACY:\",metrics.accuracy_score(y_res_test_new,y_pred_randforest))\n",
    "print('RandForest CLASSIFIER CONFUSTION MATRIX:',pd.crosstab(y_res_test_new,y_pred_randforest))\n",
    "print(\"===> **RandForest F1 SCORE\",metrics.f1_score(y_res_test_new, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest Recall SCORE\",metrics.recall_score(y_res_test_new, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest Precision SCORE\",metrics.precision_score(y_res_test_new, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest ROC_AUC SCORE\",metrics.roc_auc_score(y_res_test_new, y_pred_randforest,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for TrainingSet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset2 = pd.read_excel(path+'\\\\Trainingset2.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'filename', 'text', 'leg'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    38654\n",
       "1     5575\n",
       "Name: leg, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset2.leg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainingset2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Parsed......................\n",
      "\n",
      "Wall time: 0 ns\n",
      "Cleaned Short Descriptions 5000 of 44229\n",
      "Cleaned Short Descriptions 10000 of 44229\n",
      "Cleaned Short Descriptions 15000 of 44229\n",
      "Cleaned Short Descriptions 20000 of 44229\n",
      "Cleaned Short Descriptions 25000 of 44229\n",
      "Cleaned Short Descriptions 30000 of 44229\n",
      "Cleaned Short Descriptions 35000 of 44229\n",
      "Cleaned Short Descriptions 40000 of 44229\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows Parsed......................\\n\")\n",
    "corpus = []\n",
    "num_SD = len(X)\n",
    "%time\n",
    "for i in range( 0, num_SD ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Cleaned Short Descriptions %d of %d\" % ( i+1, num_SD ))\n",
    "    corpus.append( review_to_words(str(X.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44229"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.DataFrame(corpus,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please inform principle reciprocity rule requi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>june follow year september follow year june fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>particular asian company must residence seat f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claim must also comply certain specific admini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>country also require company country residence...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  please inform principle reciprocity rule requi...\n",
       "1  june follow year september follow year june fo...\n",
       "2  particular asian company must residence seat f...\n",
       "3  claim must also comply certain specific admini...\n",
       "4  country also require company country residence..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.concat([df_corpus,trainingset2['leg']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-fddb4f91fe98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test_txt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The tfidf vector is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "X_test_txt = tfidf_vector.transform(df_cleaned.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_test = X_test_txt.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_test = np.array(trainingset2.leg)\n",
    "state_test = state_test.reshape(-1,1)\n",
    "final_test_dtm = np.append(dense_test,state_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44229, 3001)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "test_path = r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1 = pickle.load(open(test_path+'\\RF_1.sav', 'rb'))\n",
    "rf_2 = pickle.load(open(test_path+'\\RF_2.sav', 'rb'))\n",
    "rf_3 = pickle.load(open(test_path+'\\RF_3.sav', 'rb'))\n",
    "rf_4 = pickle.load(open(test_path+'\\RF_4.sav', 'rb'))\n",
    "rf_5 = pickle.load(open(test_path+'\\RF_5.sav', 'rb'))\n",
    "rf_6 = pickle.load(open(test_path+'\\RF_6.sav', 'rb'))\n",
    "rf_7 = pickle.load(open(test_path+'\\RF_7.sav', 'rb'))\n",
    "rf_8 = pickle.load(open(test_path+'\\RF_8.sav', 'rb'))\n",
    "rf_9 = pickle.load(open(test_path+'\\RF_9.sav', 'rb'))\n",
    "rf_10 = pickle.load(open(test_path+'\\RF_10.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model1 = rf_1.predict_proba(final_test_dtm)\n",
    "y_pred_model2 = rf_2.predict_proba(final_test_dtm)\n",
    "y_pred_model3 = rf_3.predict_proba(final_test_dtm)\n",
    "y_pred_model4 = rf_4.predict_proba(final_test_dtm)\n",
    "y_pred_model5 = rf_5.predict_proba(final_test_dtm)\n",
    "y_pred_model6 = rf_6.predict_proba(final_test_dtm)\n",
    "y_pred_model7 = rf_7.predict_proba(final_test_dtm)\n",
    "y_pred_model8 = rf_8.predict_proba(final_test_dtm)\n",
    "y_pred_model9 = rf_9.predict_proba(final_test_dtm)\n",
    "y_pred_model10 = rf_10.predict_proba(final_test_dtm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n=np.vstack((y_pred_model1[:,0],y_pred_model2[:,0],y_pred_model3[:,0],y_pred_model4[:,0],y_pred_model5[:,0],\n",
    "             y_pred_model6[:,0],y_pred_model7[:,0],\n",
    "            y_pred_model8[:,0],y_pred_model9[:,0],y_pred_model10[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 44229)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92391304, 0.31      , 0.39      , ..., 0.94      , 0.89      ,\n",
       "        0.98517241],\n",
       "       [0.31      , 0.3       , 0.39      , ..., 0.9       , 0.87      ,\n",
       "        0.96      ],\n",
       "       [0.78      , 0.25      , 0.24      , ..., 0.98      , 0.98      ,\n",
       "        0.94      ],\n",
       "       ...,\n",
       "       [0.77      , 0.51      , 0.4575    , ..., 0.97      , 0.75      ,\n",
       "        0.78      ],\n",
       "       [0.9       , 0.37      , 0.33      , ..., 0.92      , 1.        ,\n",
       "        0.87      ],\n",
       "       [0.91      , 0.38      , 0.36      , ..., 0.98      , 0.94      ,\n",
       "        0.98132203]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[0:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n.transpose()\n",
    "final_df = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_2nd = pickle.load(open(test_path+'\\RF_2nd_layer.sav', 'rb'))\n",
    "final_pred = rf_2nd.predict(final_df)\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.DataFrame(final_pred,columns=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44229, 4)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44229, 1)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset2_predicted = pd.concat([trainingset2,final_pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44229, 5)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset2_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset2_predicted.to_csv('trainingset2_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38647</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred      0     1\n",
       "leg              \n",
       "0     38647     7\n",
       "1         0  5575"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(trainingset2_predicted.leg,trainingset2_predicted.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311, 3001)\n",
      "RandForest ACCURACY: 0.7877813504823151\n",
      "RandForest CLASSIFIER CONFUSTION MATRIX: col_0    0    1\n",
      "Label          \n",
      "0      128   39\n",
      "1       27  117\n",
      "===> **RandForest F1 SCORE [0.79503106 0.78      ]\n",
      "===> **RandForest Recall SCORE [0.76646707 0.8125    ]\n",
      "===> **RandForest Precision SCORE [0.82580645 0.75      ]\n",
      "===> **RandForest ROC_AUC SCORE 0.7894835329341318\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "Non_Leg = trainingset1[trainingset1['Label']== 0].sample(800,random_state=sam,replace=False)\n",
    "\n",
    "Leg = trainingset1[trainingset1['Label']==1]\n",
    "df_sampled = pd.concat([Non_Leg,Leg],axis=0)\n",
    "\n",
    "X = df_sampled.text\n",
    "y = df_sampled.Label\n",
    "\n",
    "\n",
    "\n",
    "X_train_txt, X_test_txt, y_train, y_test,state_train,state_test = train_test_split(df_sampled.text,df_sampled.Label,\n",
    "                                                                                   df_sampled.leg,test_size=0.2, random_state=1)\n",
    "X_dtm_train = tfidf_vector.fit_transform((X_train_txt.values.astype('U')))\n",
    "state_train = np.array(state_train)\n",
    "\n",
    "dense = X_dtm_train.todense()\n",
    "state_train = state_train.reshape(-1,1)\n",
    "final_train_dtm = np.append(dense,state_train,1)\n",
    "\n",
    "#print(X_dtm_train.shape)\n",
    "X_dtm_test = tfidf_vector.transform(X_test_txt.values.astype('U'))\n",
    "dense_test = X_dtm_test.todense()\n",
    "state_test = np.array(state_test)\n",
    "state_test = state_test.reshape(-1,1)\n",
    "final_test_dtm = np.append(dense_test,state_test,1)\n",
    "print(final_test_dtm.shape)\n",
    "#svcclassifier = SVC(random_state=0,kernel='linear',C=3,probability=True)\n",
    "#svcclassifier.fit(X_dtm_train,y_train)\n",
    "#y_pred_svm = svcclassifier.predict(X_dtm_test)\n",
    "\n",
    "rf_classifer = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=0,class_weight={0:.3,1:.7})\n",
    "rf_classifer.fit(final_train_dtm ,y_train)\n",
    "y_pred_randforest = rf_classifer.predict(final_test_dtm)\n",
    "\n",
    "print(\"RandForest ACCURACY:\",metrics.accuracy_score(y_test,y_pred_randforest))\n",
    "print('RandForest CLASSIFIER CONFUSTION MATRIX:',pd.crosstab(y_test,y_pred_randforest))\n",
    "print(\"===> **RandForest F1 SCORE\",metrics.f1_score(y_test, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest Recall SCORE\",metrics.recall_score(y_test, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest Precision SCORE\",metrics.precision_score(y_test, y_pred_randforest,average=None))\n",
    "print(\"===> **RandForest ROC_AUC SCORE\",metrics.roc_auc_score(y_test, y_pred_randforest,average=None))\n",
    "\n",
    "#logreg = LogisticRegression(C=1e9)\n",
    "#logreg.fit(X_dtm_train,y_train)\n",
    "#y_pred_class_logreg = logreg.predict(X_dtm_test)\n",
    "#rf_1 = \n",
    "\n",
    "#svcmodel = 'SVC'+'_'+str(sam)+'.sav'\n",
    "#pickle.dump(svcclassifier,open(svcmodel,'wb'))\n",
    "\n",
    "#rand = 'RF'+'_'+str(sam)+'.sav'\n",
    "#pickle.dump(rf_classifer,open(rand,'wb'))\n",
    "\n",
    "#log = 'Log'+'_'+str(sam)+'.sav'\n",
    "#pickle.dump(logreg,open(log,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = 'vectorizer2.pk'\n",
    "pickle.dump(tfidf_vector,open(vectorizer2,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_rf2 = rf_classifer.predict_proba(final_test_dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.DataFrame(y_pred_proba_rf1[:,0],columns=['rf1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_rf2 = pd.DataFrame(y_pred_proba_rf2[:,0],columns=['rf10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.concat([y_pred_proba,y_pred_proba_rf2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf1</th>\n",
       "      <th>rf2</th>\n",
       "      <th>rf3</th>\n",
       "      <th>rf4</th>\n",
       "      <th>rf5</th>\n",
       "      <th>rf6</th>\n",
       "      <th>rf7</th>\n",
       "      <th>rf8</th>\n",
       "      <th>rf9</th>\n",
       "      <th>rf10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.464615</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.272432</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.344615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rf1   rf2       rf3   rf4   rf5   rf6       rf7   rf8   rf9      rf10\n",
       "0  0.96  0.15  0.470000  0.97  0.97  0.91  0.270000  0.85  1.00  0.690000\n",
       "1  0.87  0.93  0.870000  0.52  1.00  0.82  1.000000  0.75  0.19  0.940000\n",
       "2  0.76  0.84  1.000000  0.56  0.67  0.86  0.990000  0.92  0.88  0.640000\n",
       "3  0.24  0.29  0.464615  0.27  0.29  0.32  0.272432  0.30  0.32  0.344615\n",
       "4  0.24  0.25  0.220000  0.27  0.25  0.19  0.240000  0.14  0.22  0.160000"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prob = y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 10)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml = []\n",
    "lab = []\n",
    "for j in output[0]:\n",
    "    for i in j:\n",
    "        l.append(i[0])\n",
    "for k in output[1]:\n",
    "    for m in k:\n",
    "        lab.append(m)\n",
    "#(output[1][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madhan.s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing data sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_prob, X_test_prob, y_trainp, y_testp, = train_test_split(X_prob, y_p,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rand_2nd_layer = RandomForestClassifier(n_estimators=200,criterion='entropy',random_state=0,class_weight={0:.4,1:0.6},max_depth=200)\n",
    "rand_2nd_layer.fit(X_train_prob,y_trainp)\n",
    "y_pred_2ndlayer = rand_2nd_layer.predict(X_test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 10)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################################################\n",
      "RandForest ACCURACY: 1.0\n",
      "RandForest CLASSIFIER CONFUSTION MATRIX: col_0   0   1\n",
      "Label        \n",
      "0      36   0\n",
      "1       0  27\n",
      "===> **RandForest F1 SCORE [1. 1.]\n",
      "===> **RandForest Recall SCORE [1. 1.]\n",
      "===> **RandForest Precision SCORE [1. 1.]\n",
      "===> **RandForest ROC_AUC SCORE 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"##################################################################################################\")\n",
    "print(\"RandForest ACCURACY:\",metrics.accuracy_score(y_testp,y_pred_2ndlayer))\n",
    "print('RandForest CLASSIFIER CONFUSTION MATRIX:',pd.crosstab(y_testp,y_pred_2ndlayer))\n",
    "print(\"===> **RandForest F1 SCORE\",metrics.f1_score(y_testp, y_pred_2ndlayer,average=None))\n",
    "print(\"===> **RandForest Recall SCORE\",metrics.recall_score(y_testp, y_pred_2ndlayer,average=None))\n",
    "print(\"===> **RandForest Precision SCORE\",metrics.precision_score(y_testp, y_pred_2ndlayer,average=None))\n",
    "print(\"===> **RandForest ROC_AUC SCORE\",metrics.roc_auc_score(y_testp, y_pred_2ndlayer,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_2 = 'RF_2nd_layer.sav'\n",
    "pickle.dump(rand_2nd_layer,open(rand_2,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement'\n",
    "pymu_test = pd.read_excel(test_path+'\\Label1_Pymupdf_TokenizedSentences.xlsx',sheet_name='Sheet1')\n",
    "bptool_files = pd.read_csv(r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement\\bp_tool_files_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1004168_D7514_HRT Placemat_111816_vFINAL - Cop...</td>\n",
       "      <td>The Next HR Transformation The Business Led HR...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1004168_D7514_HRT Placemat_111816_vFINAL - Cop...</td>\n",
       "      <td>HR strategist risk advisor talent manager Virt...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1004168_D7514_HRT Placemat_111816_vFINAL - Cop...</td>\n",
       "      <td>All Rights Reserved.</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1004168_D7514_HRT Placemat_111816_vFINAL - Cop...</td>\n",
       "      <td>ED None.</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1004168_D7514_HRT Placemat_111816_vFINAL - Cop...</td>\n",
       "      <td>EYG no.</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                           filename  \\\n",
       "0      0  1004168_D7514_HRT Placemat_111816_vFINAL - Cop...   \n",
       "1      1  1004168_D7514_HRT Placemat_111816_vFINAL - Cop...   \n",
       "2      2  1004168_D7514_HRT Placemat_111816_vFINAL - Cop...   \n",
       "3      3  1004168_D7514_HRT Placemat_111816_vFINAL - Cop...   \n",
       "4      4  1004168_D7514_HRT Placemat_111816_vFINAL - Cop...   \n",
       "\n",
       "                                                text keyword  \n",
       "0  The Next HR Transformation The Business Led HR...      No  \n",
       "1  HR strategist risk advisor talent manager Virt...     yes  \n",
       "2                               All Rights Reserved.      No  \n",
       "3                                           ED None.      No  \n",
       "4                                            EYG no.      No  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptool_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madhan.s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "bptool_files['leg'] = ''\n",
    "leg = []\n",
    "for idx, row in bptool_files.iterrows():\n",
    "    line_txt = bptool_files.text[idx]\n",
    "    #print(line_txt)\n",
    "    for row1 in LGL_words.LGL:\n",
    "        #print(row1)\n",
    "        if row1 in line_txt:\n",
    "            bptool_files['leg'][idx] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       727\n",
       "yes    188\n",
       "Name: leg, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptool_files.leg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptool_files.leg.replace(['','yes'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>leg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>910</td>\n",
       "      <td>Why a robust production system is a prerequisi...</td>\n",
       "      <td>Over time employee internalize habit important...</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>911</td>\n",
       "      <td>Why a robust production system is a prerequisi...</td>\n",
       "      <td>When enough people develop behavior affect lar...</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>912</td>\n",
       "      <td>Why a robust production system is a prerequisi...</td>\n",
       "      <td>That result company seek become sustainable fo...</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>913</td>\n",
       "      <td>Why a robust production system is a prerequisi...</td>\n",
       "      <td>A production system eliminates traditional dis...</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>914</td>\n",
       "      <td>Why a robust production system is a prerequisi...</td>\n",
       "      <td>Done right make continually improve business s...</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index                                           filename  \\\n",
       "910    910  Why a robust production system is a prerequisi...   \n",
       "911    911  Why a robust production system is a prerequisi...   \n",
       "912    912  Why a robust production system is a prerequisi...   \n",
       "913    913  Why a robust production system is a prerequisi...   \n",
       "914    914  Why a robust production system is a prerequisi...   \n",
       "\n",
       "                                                  text keyword  leg  \n",
       "910  Over time employee internalize habit important...      No    0  \n",
       "911  When enough people develop behavior affect lar...      No    0  \n",
       "912  That result company seek become sustainable fo...      No    0  \n",
       "913  A production system eliminates traditional dis...      No    0  \n",
       "914  Done right make continually improve business s...      No    0  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptool_files.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pymu = bptool_files.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915,)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pymu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Parsed......................\n",
      "\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows Parsed......................\\n\")\n",
    "corpus = []\n",
    "num_SD = len(X_test_pymu)\n",
    "%time\n",
    "for i in range( 0, num_SD ):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "        print(\"Cleaned Short Descriptions %d of %d\" % ( i+1, num_SD ))\n",
    "    corpus.append( review_to_words(str(X_test_pymu.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HR strategist risk advisor talent manager Virtual team align HR business leader address problem develop solution Innovation center share service design improve quality speed decrease cost Integrated HR business predictive analytics drive business strategy organizational decision make Globally consistent yet locally sensitive program policy procedure ensure compliance people engagement Scalable preference base mobile social system technology tool facilitate service delivery networking employee engagement framework guide transformation decision make EYGM Limited.'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pymu.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strategist risk advisor talent manager virtual team align business leader address problem develop solution innovation center share service design improve quality speed decrease cost integrate business predictive analytics drive business strategy organizational decision make globally consistent yet locally sensitive program policy procedure ensure compliance people engagement scalable preference base mobile social system technology tool facilitate service delivery networking employee engagement framework guide transformation decision make eygm limited'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testcleaned = pd.DataFrame(corpus)\n",
    "X_dtm_test = tfidf_vector.transform(X_testcleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 3000)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915,)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptool_files.leg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_test = X_dtm_test.todense()\n",
    "state_test = np.array(bptool_files.leg)\n",
    "state_test = state_test.reshape(-1,1)\n",
    "final_test_dtm = np.append(dense_test,state_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 3001)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1 = pickle.load(open(test_path+'\\RF_1.sav', 'rb'))\n",
    "rf_2 = pickle.load(open(test_path+'\\RF_2.sav', 'rb'))\n",
    "rf_3 = pickle.load(open(test_path+'\\RF_3.sav', 'rb'))\n",
    "rf_4 = pickle.load(open(test_path+'\\RF_4.sav', 'rb'))\n",
    "rf_5 = pickle.load(open(test_path+'\\RF_5.sav', 'rb'))\n",
    "rf_6 = pickle.load(open(test_path+'\\RF_6.sav', 'rb'))\n",
    "rf_7 = pickle.load(open(test_path+'\\RF_7.sav', 'rb'))\n",
    "rf_8 = pickle.load(open(test_path+'\\RF_8.sav', 'rb'))\n",
    "rf_9 = pickle.load(open(test_path+'\\RF_9.sav', 'rb'))\n",
    "rf_10 = pickle.load(open(test_path+'\\RF_10.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model1 = rf_1.predict_proba(final_test_dtm)\n",
    "y_pred_model2 = rf_2.predict_proba(final_test_dtm)\n",
    "y_pred_model3 = rf_3.predict_proba(final_test_dtm)\n",
    "y_pred_model4 = rf_4.predict_proba(final_test_dtm)\n",
    "y_pred_model5 = rf_5.predict_proba(final_test_dtm)\n",
    "y_pred_model6 = rf_6.predict_proba(final_test_dtm)\n",
    "y_pred_model7 = rf_7.predict_proba(final_test_dtm)\n",
    "y_pred_model8 = rf_8.predict_proba(final_test_dtm)\n",
    "y_pred_model9 = rf_9.predict_proba(final_test_dtm)\n",
    "y_pred_model10 = rf_10.predict_proba(final_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.vstack((y_pred_model1[:,0],y_pred_model2[:,0],y_pred_model3[:,0],y_pred_model4[:,0],y_pred_model5[:,0],\n",
    "             y_pred_model6[:,0],y_pred_model7[:,0],\n",
    "            y_pred_model8[:,0],y_pred_model9[:,0],y_pred_model10[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 915)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86      , 0.33      , 0.98      , ..., 0.94      , 1.        ,\n",
       "        0.873     ],\n",
       "       [0.84024038, 0.30194079, 0.99      , ..., 0.92      , 0.85      ,\n",
       "        0.86      ],\n",
       "       [0.56      , 0.17593117, 0.99      , ..., 0.82      , 0.64      ,\n",
       "        0.58      ],\n",
       "       ...,\n",
       "       [0.83      , 0.38      , 1.        , ..., 0.97      , 0.87      ,\n",
       "        0.95      ],\n",
       "       [0.9       , 0.43631579, 0.94      , ..., 0.95      , 0.94517241,\n",
       "        0.92      ],\n",
       "       [0.38      , 0.13461538, 1.        , ..., 0.81      , 0.68      ,\n",
       "        0.58      ]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[0:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2nd = pickle.load(open(test_path+'\\RF_2nd_layer.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = rf_2nd.predict(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.DataFrame(final_pred,columns=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testcleaned = pd.concat([X_testcleaned,final_pred,bptool_files.filename],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915,)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testcleaned.filename.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915, 3)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testcleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_testcleaned.iloc[6321]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testcleaned.to_csv(r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement\\bptoolfiles_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>leg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>Please inform principle reciprocity rule requi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>June follow year September follow year June fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>In particular Asian company must Not residence...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>Claims must also comply certain specific admin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>Some EU country also require company country r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                   filename  \\\n",
       "0      0  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "1      1  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "2      2  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "3      3  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "4      4  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "\n",
       "                                                text  leg  \n",
       "0  Please inform principle reciprocity rule requi...    0  \n",
       "1  June follow year September follow year June fo...    1  \n",
       "2  In particular Asian company must Not residence...    1  \n",
       "3  Claims must also comply certain specific admin...    0  \n",
       "4  Some EU country also require company country r...    0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30801, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainingset1.text\n",
    "y = trainingset1.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Leg = trainingset1[trainingset1['Label']== 0].sample(800,random_state=sam,replace=False)\n",
    "\n",
    "Leg = trainingset1[trainingset1['Label']==1]\n",
    "df_sampled = pd.concat([Non_Leg,Leg],axis=0)\n",
    "\n",
    "X = df_sampled.text\n",
    "y = df_sampled.Label\n",
    "\n",
    "\n",
    "\n",
    "X_train_txt, X_test_txt, y_train, y_test,state_train,state_test = train_test_split(df_sampled.text,df_sampled.Label,\n",
    "                                                                                   df_sampled.leg,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1244,)\n",
      "(311,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_txt.shape)\n",
    "print(X_test_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 3000\n",
    "max_len = 300\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train_txt.values.astype('U'))\n",
    "sequences = tok.texts_to_sequences(X_train_txt.values.astype('U'))\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tok.texts_to_sequences(X_test_txt.values.astype('U'))\n",
    "sequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   57  371   49    7  216  174   10   41\n",
      "  111  372  644 1482  502  731]\n"
     ]
    }
   ],
   "source": [
    "print(sequences_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3\n",
    "\n",
    "word_id = {k:(v+INDEX_FROM) for k,v in word_id.items()} # Shift all the items to make space for the SPECIAL WORDS\n",
    "word_id[\"_UNK_\"] = 0\n",
    "word_id[\"_START_\"] = 1\n",
    "word_id[\"_CUT_\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_word = {v:k for k,v in word_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_CUT_'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   17, 1483,  645,   88, 1062,    5,  188,\n",
       "         19,  394,  934])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ help deduction light controversy guide com ifrs client beps linkedin\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_word[i] for i in sequences_matrix[1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7507f6cf0b7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "vanilla_model = Sequential()\n",
    "vanilla_model.add(Embedding(3000, 500, input_length=max_len)) #vOCABSIZE IS 5000\n",
    "#vanilla_model.add(BatchNormalization())\n",
    "vanilla_model.add(Flatten())\n",
    "vanilla_model.add(Dense(250, activation='relu'))\n",
    "vanilla_model.add(Dropout(0.2))\n",
    "vanilla_model.add(Dense(1, activation='sigmoid'))\n",
    "vanilla_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 300, 500)          1500000   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 150000)            0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 250)               37500250  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 39,000,501\n",
      "Trainable params: 39,000,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1244 samples, validate on 311 samples\n",
      "Epoch 1/50\n",
      " - 29s - loss: 1.4237 - acc: 0.5675 - val_loss: 0.5827 - val_acc: 0.6881\n",
      "Epoch 2/50\n",
      " - 25s - loss: 0.5083 - acc: 0.7540 - val_loss: 0.5110 - val_acc: 0.7395\n",
      "Epoch 3/50\n",
      " - 28s - loss: 0.2810 - acc: 0.9092 - val_loss: 0.4783 - val_acc: 0.7942\n",
      "Epoch 4/50\n",
      " - 28s - loss: 0.1380 - acc: 0.9654 - val_loss: 0.4520 - val_acc: 0.8199\n",
      "Epoch 5/50\n",
      " - 25s - loss: 0.0671 - acc: 0.9871 - val_loss: 0.4698 - val_acc: 0.8167\n",
      "Epoch 6/50\n",
      " - 26s - loss: 0.0416 - acc: 0.9920 - val_loss: 0.4766 - val_acc: 0.8103\n",
      "Epoch 7/50\n",
      " - 26s - loss: 0.0317 - acc: 0.9912 - val_loss: 0.4825 - val_acc: 0.8199\n",
      "Epoch 8/50\n",
      " - 26s - loss: 0.0269 - acc: 0.9920 - val_loss: 0.5268 - val_acc: 0.7846\n",
      "Epoch 9/50\n",
      " - 28s - loss: 0.0213 - acc: 0.9928 - val_loss: 0.5038 - val_acc: 0.8071\n",
      "Epoch 10/50\n",
      " - 25s - loss: 0.0217 - acc: 0.9928 - val_loss: 0.5283 - val_acc: 0.8232\n",
      "Epoch 11/50\n",
      " - 26s - loss: 0.0204 - acc: 0.9928 - val_loss: 0.5222 - val_acc: 0.8103\n",
      "Epoch 12/50\n",
      " - 25s - loss: 0.0209 - acc: 0.9944 - val_loss: 0.5858 - val_acc: 0.7781\n",
      "Epoch 13/50\n",
      " - 23s - loss: 0.0201 - acc: 0.9952 - val_loss: 0.5445 - val_acc: 0.8071\n",
      "Epoch 14/50\n",
      " - 24s - loss: 0.0206 - acc: 0.9936 - val_loss: 0.5406 - val_acc: 0.8071\n",
      "Epoch 15/50\n",
      " - 24s - loss: 0.0211 - acc: 0.9928 - val_loss: 0.5394 - val_acc: 0.8135\n",
      "Epoch 16/50\n",
      " - 23s - loss: 0.0209 - acc: 0.9912 - val_loss: 0.5562 - val_acc: 0.8006\n",
      "Epoch 17/50\n",
      " - 22s - loss: 0.0191 - acc: 0.9936 - val_loss: 0.5591 - val_acc: 0.8135\n",
      "Epoch 18/50\n",
      " - 22s - loss: 0.0196 - acc: 0.9944 - val_loss: 0.5498 - val_acc: 0.8135\n",
      "Epoch 19/50\n",
      " - 22s - loss: 0.0153 - acc: 0.9928 - val_loss: 0.5565 - val_acc: 0.8006\n",
      "Epoch 20/50\n",
      " - 22s - loss: 0.0168 - acc: 0.9904 - val_loss: 0.6304 - val_acc: 0.7781\n",
      "Epoch 21/50\n",
      " - 22s - loss: 0.0205 - acc: 0.9944 - val_loss: 0.5725 - val_acc: 0.8167\n",
      "Epoch 22/50\n",
      " - 23s - loss: 0.0135 - acc: 0.9960 - val_loss: 0.5594 - val_acc: 0.8135\n",
      "Epoch 23/50\n",
      " - 21s - loss: 0.0121 - acc: 0.9944 - val_loss: 0.5681 - val_acc: 0.8103\n",
      "Epoch 24/50\n",
      " - 21s - loss: 0.0176 - acc: 0.9944 - val_loss: 0.6031 - val_acc: 0.8039\n",
      "Epoch 25/50\n",
      " - 21s - loss: 0.0192 - acc: 0.9920 - val_loss: 0.6173 - val_acc: 0.7814\n",
      "Epoch 26/50\n",
      " - 21s - loss: 0.0217 - acc: 0.9912 - val_loss: 0.5660 - val_acc: 0.8006\n",
      "Epoch 27/50\n",
      " - 21s - loss: 0.0207 - acc: 0.9944 - val_loss: 0.5705 - val_acc: 0.8039\n",
      "Epoch 28/50\n",
      " - 21s - loss: 0.0163 - acc: 0.9928 - val_loss: 0.6148 - val_acc: 0.7814\n",
      "Epoch 29/50\n",
      " - 21s - loss: 0.0177 - acc: 0.9952 - val_loss: 0.5646 - val_acc: 0.8039\n",
      "Epoch 30/50\n",
      " - 21s - loss: 0.0165 - acc: 0.9912 - val_loss: 0.5708 - val_acc: 0.7942\n",
      "Epoch 31/50\n",
      " - 21s - loss: 0.0172 - acc: 0.9920 - val_loss: 0.5744 - val_acc: 0.7974\n",
      "Epoch 32/50\n",
      " - 24s - loss: 0.0145 - acc: 0.9920 - val_loss: 0.6150 - val_acc: 0.7910\n",
      "Epoch 33/50\n",
      " - 22s - loss: 0.0149 - acc: 0.9952 - val_loss: 0.5846 - val_acc: 0.7878\n",
      "Epoch 34/50\n",
      " - 22s - loss: 0.0166 - acc: 0.9912 - val_loss: 0.6073 - val_acc: 0.7974\n",
      "Epoch 35/50\n",
      " - 22s - loss: 0.0148 - acc: 0.9952 - val_loss: 0.6214 - val_acc: 0.7910\n",
      "Epoch 36/50\n",
      " - 21s - loss: 0.0129 - acc: 0.9952 - val_loss: 0.6066 - val_acc: 0.7974\n",
      "Epoch 37/50\n",
      " - 20s - loss: 0.0156 - acc: 0.9944 - val_loss: 0.6312 - val_acc: 0.7910\n",
      "Epoch 38/50\n",
      " - 21s - loss: 0.0148 - acc: 0.9952 - val_loss: 0.5872 - val_acc: 0.7942\n",
      "Epoch 39/50\n",
      " - 21s - loss: 0.0156 - acc: 0.9887 - val_loss: 0.6090 - val_acc: 0.7942\n",
      "Epoch 40/50\n",
      " - 21s - loss: 0.0170 - acc: 0.9952 - val_loss: 0.5955 - val_acc: 0.7878\n",
      "Epoch 41/50\n",
      " - 21s - loss: 0.0153 - acc: 0.9936 - val_loss: 0.6183 - val_acc: 0.7878\n",
      "Epoch 42/50\n",
      " - 23s - loss: 0.0167 - acc: 0.9895 - val_loss: 0.6342 - val_acc: 0.7878\n",
      "Epoch 43/50\n",
      " - 23s - loss: 0.0157 - acc: 0.9952 - val_loss: 0.5930 - val_acc: 0.7846\n",
      "Epoch 44/50\n",
      " - 22s - loss: 0.0119 - acc: 0.9952 - val_loss: 0.6491 - val_acc: 0.7781\n",
      "Epoch 45/50\n",
      " - 22s - loss: 0.0149 - acc: 0.9944 - val_loss: 0.5964 - val_acc: 0.7942\n",
      "Epoch 46/50\n",
      " - 24s - loss: 0.0161 - acc: 0.9920 - val_loss: 0.6547 - val_acc: 0.7846\n",
      "Epoch 47/50\n",
      " - 26s - loss: 0.0127 - acc: 0.9952 - val_loss: 0.6286 - val_acc: 0.7878\n",
      "Epoch 48/50\n",
      " - 30s - loss: 0.0142 - acc: 0.9944 - val_loss: 0.6195 - val_acc: 0.7878\n",
      "Epoch 49/50\n",
      " - 29s - loss: 0.0148 - acc: 0.9912 - val_loss: 0.6170 - val_acc: 0.7846\n",
      "Epoch 50/50\n",
      " - 32s - loss: 0.0147 - acc: 0.9952 - val_loss: 0.6578 - val_acc: 0.7846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2335b3d10f0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model.fit(sequences_matrix, y_train, batch_size=64, epochs=50,\n",
    "          validation_data=(sequences_matrix_test, y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NN = vanilla_model.predict_classes(sequences_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NN = pd.DataFrame(pred_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151,  16],\n",
       "       [ 47,  97]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,pred_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN ACCURACY: 0.797427652733119\n",
      "NN CLASSIFIER CONFUSTION MATRIX: [[151  16]\n",
      " [ 47  97]]\n",
      "===> **NN F1 SCORE [0.82739726 0.75486381]\n",
      "===> **NN Recall SCORE [0.90419162 0.67361111]\n",
      "===> **NN Precision SCORE [0.76262626 0.85840708]\n",
      "===> **NN ROC_AUC SCORE 0.7889013639387891\n",
      "#######################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"NN ACCURACY:\",metrics.accuracy_score(y_test,pred_NN))\n",
    "print('NN CLASSIFIER CONFUSTION MATRIX:',metrics.confusion_matrix(y_test,pred_NN))\n",
    "print(\"===> **NN F1 SCORE\",metrics.f1_score(y_test, pred_NN,average=None))\n",
    "print(\"===> **NN Recall SCORE\",metrics.recall_score(y_test, pred_NN,average=None))\n",
    "print(\"===> **NN Precision SCORE\",metrics.precision_score(y_test, pred_NN,average=None))\n",
    "print(\"===> **NN ROC_AUC SCORE\",metrics.roc_auc_score(y_test, pred_NN,average=None))\n",
    "print(\"#######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(X_train_txt.astype('U'))\n",
    "sequences_train = tok.texts_to_sequences(X_train_txt.astype('U'))\n",
    "sequences_test = tok.texts_to_sequences(X_test_txt)\n",
    "#load dataset as a list of ints\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "x_test =  sequence.pad_sequences(sequences_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(vocab_size, 128))\n",
    "lstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#lstm_model.add(\n",
    "lstm_model.add(Dense(100, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                                         patience=2, verbose=0, mode='auto')\n",
    "\n",
    "# Compile and train.\n",
    "lstm_model.compile(loss='binary_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24640 samples, validate on 6161 samples\n",
      "Epoch 1/2\n",
      "12992/24640 [==============>...............] - ETA: 29:03 - loss: 0.5374 - acc: 0.98 - ETA: 30:28 - loss: 0.5411 - acc: 0.96 - ETA: 30:12 - loss: 0.5362 - acc: 0.97 - ETA: 30:08 - loss: 0.5326 - acc: 0.98 - ETA: 29:53 - loss: 0.5303 - acc: 0.98 - ETA: 29:33 - loss: 0.5268 - acc: 0.98 - ETA: 29:40 - loss: 0.5259 - acc: 0.98 - ETA: 29:59 - loss: 0.5244 - acc: 0.98 - ETA: 30:04 - loss: 0.5227 - acc: 0.98 - ETA: 30:05 - loss: 0.5217 - acc: 0.98 - ETA: 30:03 - loss: 0.5204 - acc: 0.98 - ETA: 29:58 - loss: 0.5193 - acc: 0.98 - ETA: 29:43 - loss: 0.5168 - acc: 0.98 - ETA: 29:43 - loss: 0.5159 - acc: 0.97 - ETA: 29:39 - loss: 0.5136 - acc: 0.98 - ETA: 29:35 - loss: 0.5122 - acc: 0.98 - ETA: 29:32 - loss: 0.5099 - acc: 0.98 - ETA: 29:29 - loss: 0.5086 - acc: 0.98 - ETA: 29:27 - loss: 0.5063 - acc: 0.98 - ETA: 29:29 - loss: 0.5043 - acc: 0.98 - ETA: 29:24 - loss: 0.5026 - acc: 0.98 - ETA: 29:20 - loss: 0.5013 - acc: 0.98 - ETA: 29:18 - loss: 0.4999 - acc: 0.98 - ETA: 29:35 - loss: 0.4988 - acc: 0.98 - ETA: 29:25 - loss: 0.4982 - acc: 0.97 - ETA: 29:18 - loss: 0.4983 - acc: 0.97 - ETA: 29:09 - loss: 0.4962 - acc: 0.97 - ETA: 29:02 - loss: 0.4941 - acc: 0.97 - ETA: 28:52 - loss: 0.4935 - acc: 0.97 - ETA: 28:46 - loss: 0.4917 - acc: 0.97 - ETA: 28:42 - loss: 0.4903 - acc: 0.97 - ETA: 28:39 - loss: 0.4883 - acc: 0.97 - ETA: 28:34 - loss: 0.4866 - acc: 0.97 - ETA: 28:26 - loss: 0.4856 - acc: 0.97 - ETA: 28:20 - loss: 0.4838 - acc: 0.97 - ETA: 28:11 - loss: 0.4819 - acc: 0.97 - ETA: 28:05 - loss: 0.4805 - acc: 0.97 - ETA: 27:58 - loss: 0.4792 - acc: 0.97 - ETA: 27:52 - loss: 0.4779 - acc: 0.97 - ETA: 27:44 - loss: 0.4766 - acc: 0.97 - ETA: 27:38 - loss: 0.4751 - acc: 0.97 - ETA: 27:30 - loss: 0.4732 - acc: 0.97 - ETA: 27:24 - loss: 0.4719 - acc: 0.97 - ETA: 27:18 - loss: 0.4701 - acc: 0.97 - ETA: 27:14 - loss: 0.4688 - acc: 0.97 - ETA: 27:08 - loss: 0.4675 - acc: 0.97 - ETA: 27:02 - loss: 0.4660 - acc: 0.97 - ETA: 26:55 - loss: 0.4647 - acc: 0.97 - ETA: 26:49 - loss: 0.4631 - acc: 0.97 - ETA: 26:43 - loss: 0.4625 - acc: 0.97 - ETA: 26:39 - loss: 0.4615 - acc: 0.97 - ETA: 26:33 - loss: 0.4602 - acc: 0.97 - ETA: 26:27 - loss: 0.4588 - acc: 0.97 - ETA: 26:21 - loss: 0.4579 - acc: 0.97 - ETA: 26:15 - loss: 0.4561 - acc: 0.97 - ETA: 26:09 - loss: 0.4546 - acc: 0.97 - ETA: 26:03 - loss: 0.4530 - acc: 0.97 - ETA: 25:57 - loss: 0.4513 - acc: 0.97 - ETA: 25:52 - loss: 0.4502 - acc: 0.97 - ETA: 25:48 - loss: 0.4492 - acc: 0.97 - ETA: 25:42 - loss: 0.4483 - acc: 0.97 - ETA: 25:37 - loss: 0.4467 - acc: 0.97 - ETA: 25:31 - loss: 0.4452 - acc: 0.97 - ETA: 25:26 - loss: 0.4441 - acc: 0.97 - ETA: 25:19 - loss: 0.4426 - acc: 0.97 - ETA: 25:15 - loss: 0.4410 - acc: 0.97 - ETA: 25:10 - loss: 0.4396 - acc: 0.97 - ETA: 25:04 - loss: 0.4380 - acc: 0.97 - ETA: 24:59 - loss: 0.4368 - acc: 0.97 - ETA: 24:54 - loss: 0.4352 - acc: 0.97 - ETA: 24:48 - loss: 0.4340 - acc: 0.97 - ETA: 24:44 - loss: 0.4325 - acc: 0.97 - ETA: 24:39 - loss: 0.4314 - acc: 0.97 - ETA: 24:34 - loss: 0.4300 - acc: 0.97 - ETA: 24:28 - loss: 0.4288 - acc: 0.97 - ETA: 24:23 - loss: 0.4276 - acc: 0.97 - ETA: 24:17 - loss: 0.4264 - acc: 0.97 - ETA: 24:12 - loss: 0.4256 - acc: 0.97 - ETA: 24:07 - loss: 0.4251 - acc: 0.97 - ETA: 24:01 - loss: 0.4235 - acc: 0.97 - ETA: 23:55 - loss: 0.4228 - acc: 0.97 - ETA: 23:50 - loss: 0.4217 - acc: 0.97 - ETA: 23:44 - loss: 0.4205 - acc: 0.97 - ETA: 23:40 - loss: 0.4192 - acc: 0.97 - ETA: 23:37 - loss: 0.4185 - acc: 0.97 - ETA: 23:31 - loss: 0.4178 - acc: 0.97 - ETA: 23:27 - loss: 0.4165 - acc: 0.97 - ETA: 23:23 - loss: 0.4157 - acc: 0.97 - ETA: 23:18 - loss: 0.4144 - acc: 0.97 - ETA: 23:13 - loss: 0.4129 - acc: 0.97 - ETA: 23:07 - loss: 0.4119 - acc: 0.97 - ETA: 23:03 - loss: 0.4110 - acc: 0.97 - ETA: 22:57 - loss: 0.4099 - acc: 0.97 - ETA: 22:52 - loss: 0.4087 - acc: 0.97 - ETA: 22:47 - loss: 0.4072 - acc: 0.97 - ETA: 22:42 - loss: 0.4059 - acc: 0.97 - ETA: 22:37 - loss: 0.4047 - acc: 0.97 - ETA: 22:32 - loss: 0.4035 - acc: 0.97 - ETA: 22:27 - loss: 0.4022 - acc: 0.97 - ETA: 22:22 - loss: 0.4012 - acc: 0.97 - ETA: 22:18 - loss: 0.3999 - acc: 0.97 - ETA: 22:15 - loss: 0.3989 - acc: 0.97 - ETA: 22:10 - loss: 0.3977 - acc: 0.97 - ETA: 22:07 - loss: 0.3969 - acc: 0.97 - ETA: 22:02 - loss: 0.3962 - acc: 0.97 - ETA: 21:58 - loss: 0.3950 - acc: 0.97 - ETA: 21:55 - loss: 0.3938 - acc: 0.97 - ETA: 21:51 - loss: 0.3928 - acc: 0.97 - ETA: 21:46 - loss: 0.3916 - acc: 0.97 - ETA: 21:42 - loss: 0.3902 - acc: 0.97 - ETA: 21:40 - loss: 0.3892 - acc: 0.97 - ETA: 21:37 - loss: 0.3880 - acc: 0.97 - ETA: 21:32 - loss: 0.3872 - acc: 0.97 - ETA: 21:27 - loss: 0.3866 - acc: 0.97 - ETA: 21:23 - loss: 0.3854 - acc: 0.97 - ETA: 21:19 - loss: 0.3845 - acc: 0.97 - ETA: 21:16 - loss: 0.3835 - acc: 0.97 - ETA: 21:12 - loss: 0.3827 - acc: 0.97 - ETA: 21:07 - loss: 0.3819 - acc: 0.97 - ETA: 21:03 - loss: 0.3810 - acc: 0.97 - ETA: 20:59 - loss: 0.3799 - acc: 0.97 - ETA: 20:55 - loss: 0.3791 - acc: 0.97 - ETA: 20:51 - loss: 0.3780 - acc: 0.97 - ETA: 20:47 - loss: 0.3770 - acc: 0.97 - ETA: 20:42 - loss: 0.3760 - acc: 0.97 - ETA: 20:40 - loss: 0.3747 - acc: 0.97 - ETA: 20:40 - loss: 0.3740 - acc: 0.97 - ETA: 20:37 - loss: 0.3731 - acc: 0.97 - ETA: 20:34 - loss: 0.3722 - acc: 0.97 - ETA: 20:31 - loss: 0.3711 - acc: 0.97 - ETA: 20:31 - loss: 0.3698 - acc: 0.97 - ETA: 20:30 - loss: 0.3688 - acc: 0.97 - ETA: 20:29 - loss: 0.3682 - acc: 0.97 - ETA: 20:27 - loss: 0.3671 - acc: 0.97 - ETA: 20:26 - loss: 0.3661 - acc: 0.97 - ETA: 20:26 - loss: 0.3651 - acc: 0.97 - ETA: 20:26 - loss: 0.3642 - acc: 0.97 - ETA: 20:24 - loss: 0.3630 - acc: 0.97 - ETA: 20:19 - loss: 0.3622 - acc: 0.97 - ETA: 20:15 - loss: 0.3611 - acc: 0.97 - ETA: 20:11 - loss: 0.3603 - acc: 0.97 - ETA: 20:06 - loss: 0.3596 - acc: 0.97 - ETA: 20:02 - loss: 0.3584 - acc: 0.97 - ETA: 19:59 - loss: 0.3574 - acc: 0.97 - ETA: 19:55 - loss: 0.3566 - acc: 0.97 - ETA: 19:53 - loss: 0.3557 - acc: 0.97 - ETA: 19:52 - loss: 0.3552 - acc: 0.97 - ETA: 19:51 - loss: 0.3541 - acc: 0.97 - ETA: 19:49 - loss: 0.3532 - acc: 0.97 - ETA: 19:49 - loss: 0.3520 - acc: 0.97 - ETA: 19:50 - loss: 0.3514 - acc: 0.97 - ETA: 19:49 - loss: 0.3502 - acc: 0.97 - ETA: 19:50 - loss: 0.3494 - acc: 0.97 - ETA: 19:48 - loss: 0.3490 - acc: 0.97 - ETA: 19:45 - loss: 0.3479 - acc: 0.97 - ETA: 19:42 - loss: 0.3469 - acc: 0.97 - ETA: 19:39 - loss: 0.3462 - acc: 0.97 - ETA: 19:36 - loss: 0.3452 - acc: 0.97 - ETA: 19:31 - loss: 0.3443 - acc: 0.97 - ETA: 19:28 - loss: 0.3432 - acc: 0.97 - ETA: 19:24 - loss: 0.3424 - acc: 0.97 - ETA: 19:19 - loss: 0.3417 - acc: 0.97 - ETA: 19:14 - loss: 0.3406 - acc: 0.97 - ETA: 19:11 - loss: 0.3395 - acc: 0.97 - ETA: 19:06 - loss: 0.3383 - acc: 0.97 - ETA: 19:02 - loss: 0.3374 - acc: 0.97 - ETA: 18:57 - loss: 0.3367 - acc: 0.97 - ETA: 18:51 - loss: 0.3364 - acc: 0.97 - ETA: 18:46 - loss: 0.3359 - acc: 0.97 - ETA: 18:40 - loss: 0.3348 - acc: 0.97 - ETA: 18:35 - loss: 0.3340 - acc: 0.97 - ETA: 18:29 - loss: 0.3330 - acc: 0.97 - ETA: 18:24 - loss: 0.3323 - acc: 0.97 - ETA: 18:18 - loss: 0.3317 - acc: 0.97 - ETA: 18:12 - loss: 0.3309 - acc: 0.97 - ETA: 18:07 - loss: 0.3300 - acc: 0.97 - ETA: 18:02 - loss: 0.3293 - acc: 0.97 - ETA: 17:58 - loss: 0.3288 - acc: 0.97 - ETA: 17:52 - loss: 0.3278 - acc: 0.97 - ETA: 17:47 - loss: 0.3269 - acc: 0.97 - ETA: 17:42 - loss: 0.3262 - acc: 0.97 - ETA: 17:37 - loss: 0.3253 - acc: 0.97 - ETA: 17:31 - loss: 0.3245 - acc: 0.97 - ETA: 17:25 - loss: 0.3234 - acc: 0.97 - ETA: 17:20 - loss: 0.3226 - acc: 0.97 - ETA: 17:14 - loss: 0.3219 - acc: 0.97 - ETA: 17:09 - loss: 0.3210 - acc: 0.97 - ETA: 17:03 - loss: 0.3206 - acc: 0.97 - ETA: 16:57 - loss: 0.3199 - acc: 0.97 - ETA: 16:53 - loss: 0.3190 - acc: 0.97 - ETA: 16:47 - loss: 0.3181 - acc: 0.97 - ETA: 16:42 - loss: 0.3174 - acc: 0.97 - ETA: 16:36 - loss: 0.3164 - acc: 0.97 - ETA: 16:31 - loss: 0.3158 - acc: 0.97 - ETA: 16:25 - loss: 0.3150 - acc: 0.97 - ETA: 16:20 - loss: 0.3145 - acc: 0.97 - ETA: 16:15 - loss: 0.3137 - acc: 0.97 - ETA: 16:10 - loss: 0.3127 - acc: 0.97 - ETA: 16:04 - loss: 0.3119 - acc: 0.97 - ETA: 15:59 - loss: 0.3112 - acc: 0.97 - ETA: 15:53 - loss: 0.3102 - acc: 0.97 - ETA: 15:48 - loss: 0.3095 - acc: 0.97 - ETA: 15:43 - loss: 0.3089 - acc: 0.9724640/24640 [==============================] - ETA: 15:37 - loss: 0.3080 - acc: 0.97 - ETA: 15:32 - loss: 0.3077 - acc: 0.97 - ETA: 15:27 - loss: 0.3068 - acc: 0.97 - ETA: 15:21 - loss: 0.3060 - acc: 0.97 - ETA: 15:16 - loss: 0.3054 - acc: 0.97 - ETA: 15:11 - loss: 0.3045 - acc: 0.97 - ETA: 15:06 - loss: 0.3039 - acc: 0.97 - ETA: 15:01 - loss: 0.3032 - acc: 0.97 - ETA: 14:56 - loss: 0.3029 - acc: 0.97 - ETA: 14:50 - loss: 0.3021 - acc: 0.97 - ETA: 14:45 - loss: 0.3015 - acc: 0.97 - ETA: 14:40 - loss: 0.3010 - acc: 0.97 - ETA: 14:35 - loss: 0.3004 - acc: 0.97 - ETA: 14:30 - loss: 0.2997 - acc: 0.97 - ETA: 14:24 - loss: 0.2990 - acc: 0.97 - ETA: 14:19 - loss: 0.2986 - acc: 0.97 - ETA: 14:15 - loss: 0.2979 - acc: 0.97 - ETA: 14:09 - loss: 0.2974 - acc: 0.97 - ETA: 14:04 - loss: 0.2968 - acc: 0.97 - ETA: 13:58 - loss: 0.2966 - acc: 0.97 - ETA: 13:53 - loss: 0.2959 - acc: 0.97 - ETA: 13:50 - loss: 0.2952 - acc: 0.97 - ETA: 13:46 - loss: 0.2943 - acc: 0.97 - ETA: 13:42 - loss: 0.2935 - acc: 0.97 - ETA: 13:37 - loss: 0.2928 - acc: 0.97 - ETA: 13:31 - loss: 0.2922 - acc: 0.97 - ETA: 13:26 - loss: 0.2919 - acc: 0.97 - ETA: 13:21 - loss: 0.2912 - acc: 0.97 - ETA: 13:16 - loss: 0.2905 - acc: 0.97 - ETA: 13:11 - loss: 0.2899 - acc: 0.97 - ETA: 13:05 - loss: 0.2894 - acc: 0.97 - ETA: 13:00 - loss: 0.2892 - acc: 0.97 - ETA: 12:54 - loss: 0.2890 - acc: 0.97 - ETA: 12:49 - loss: 0.2885 - acc: 0.97 - ETA: 12:44 - loss: 0.2880 - acc: 0.97 - ETA: 12:39 - loss: 0.2878 - acc: 0.97 - ETA: 12:33 - loss: 0.2870 - acc: 0.97 - ETA: 12:28 - loss: 0.2864 - acc: 0.97 - ETA: 12:23 - loss: 0.2861 - acc: 0.97 - ETA: 12:17 - loss: 0.2855 - acc: 0.97 - ETA: 12:12 - loss: 0.2853 - acc: 0.97 - ETA: 12:06 - loss: 0.2850 - acc: 0.97 - ETA: 12:01 - loss: 0.2843 - acc: 0.97 - ETA: 11:56 - loss: 0.2838 - acc: 0.97 - ETA: 11:51 - loss: 0.2830 - acc: 0.97 - ETA: 11:46 - loss: 0.2823 - acc: 0.97 - ETA: 11:41 - loss: 0.2818 - acc: 0.97 - ETA: 11:36 - loss: 0.2812 - acc: 0.97 - ETA: 11:31 - loss: 0.2806 - acc: 0.97 - ETA: 11:26 - loss: 0.2800 - acc: 0.97 - ETA: 11:21 - loss: 0.2794 - acc: 0.97 - ETA: 11:16 - loss: 0.2786 - acc: 0.97 - ETA: 11:10 - loss: 0.2780 - acc: 0.97 - ETA: 11:05 - loss: 0.2781 - acc: 0.97 - ETA: 11:00 - loss: 0.2776 - acc: 0.97 - ETA: 10:55 - loss: 0.2773 - acc: 0.97 - ETA: 10:50 - loss: 0.2769 - acc: 0.97 - ETA: 10:45 - loss: 0.2764 - acc: 0.97 - ETA: 10:40 - loss: 0.2760 - acc: 0.97 - ETA: 10:35 - loss: 0.2757 - acc: 0.97 - ETA: 10:30 - loss: 0.2751 - acc: 0.97 - ETA: 10:25 - loss: 0.2747 - acc: 0.97 - ETA: 10:20 - loss: 0.2740 - acc: 0.97 - ETA: 10:15 - loss: 0.2735 - acc: 0.97 - ETA: 10:10 - loss: 0.2730 - acc: 0.97 - ETA: 10:05 - loss: 0.2724 - acc: 0.97 - ETA: 10:00 - loss: 0.2719 - acc: 0.97 - ETA: 9:55 - loss: 0.2717 - acc: 0.9748 - ETA: 9:49 - loss: 0.2711 - acc: 0.974 - ETA: 9:45 - loss: 0.2705 - acc: 0.974 - ETA: 9:39 - loss: 0.2700 - acc: 0.974 - ETA: 9:34 - loss: 0.2693 - acc: 0.975 - ETA: 9:29 - loss: 0.2691 - acc: 0.974 - ETA: 9:24 - loss: 0.2689 - acc: 0.974 - ETA: 9:19 - loss: 0.2687 - acc: 0.974 - ETA: 9:14 - loss: 0.2686 - acc: 0.974 - ETA: 9:09 - loss: 0.2683 - acc: 0.974 - ETA: 9:04 - loss: 0.2677 - acc: 0.974 - ETA: 8:59 - loss: 0.2673 - acc: 0.974 - ETA: 8:54 - loss: 0.2667 - acc: 0.974 - ETA: 8:49 - loss: 0.2662 - acc: 0.974 - ETA: 8:44 - loss: 0.2657 - acc: 0.974 - ETA: 8:39 - loss: 0.2650 - acc: 0.974 - ETA: 8:34 - loss: 0.2643 - acc: 0.974 - ETA: 8:29 - loss: 0.2637 - acc: 0.974 - ETA: 8:23 - loss: 0.2631 - acc: 0.975 - ETA: 8:18 - loss: 0.2625 - acc: 0.975 - ETA: 8:13 - loss: 0.2619 - acc: 0.975 - ETA: 8:08 - loss: 0.2614 - acc: 0.975 - ETA: 8:03 - loss: 0.2610 - acc: 0.975 - ETA: 7:58 - loss: 0.2606 - acc: 0.975 - ETA: 7:53 - loss: 0.2607 - acc: 0.974 - ETA: 7:48 - loss: 0.2600 - acc: 0.975 - ETA: 7:42 - loss: 0.2594 - acc: 0.975 - ETA: 7:37 - loss: 0.2590 - acc: 0.975 - ETA: 7:32 - loss: 0.2586 - acc: 0.975 - ETA: 7:27 - loss: 0.2583 - acc: 0.975 - ETA: 7:21 - loss: 0.2581 - acc: 0.974 - ETA: 7:16 - loss: 0.2575 - acc: 0.975 - ETA: 7:11 - loss: 0.2569 - acc: 0.975 - ETA: 7:06 - loss: 0.2567 - acc: 0.975 - ETA: 7:01 - loss: 0.2562 - acc: 0.975 - ETA: 6:56 - loss: 0.2556 - acc: 0.975 - ETA: 6:51 - loss: 0.2555 - acc: 0.975 - ETA: 6:45 - loss: 0.2551 - acc: 0.975 - ETA: 6:40 - loss: 0.2546 - acc: 0.975 - ETA: 6:35 - loss: 0.2542 - acc: 0.975 - ETA: 6:30 - loss: 0.2536 - acc: 0.975 - ETA: 6:24 - loss: 0.2531 - acc: 0.975 - ETA: 6:19 - loss: 0.2531 - acc: 0.975 - ETA: 6:14 - loss: 0.2525 - acc: 0.975 - ETA: 6:09 - loss: 0.2524 - acc: 0.975 - ETA: 6:04 - loss: 0.2519 - acc: 0.975 - ETA: 5:58 - loss: 0.2513 - acc: 0.975 - ETA: 5:53 - loss: 0.2507 - acc: 0.975 - ETA: 5:48 - loss: 0.2505 - acc: 0.975 - ETA: 5:42 - loss: 0.2503 - acc: 0.975 - ETA: 5:37 - loss: 0.2498 - acc: 0.975 - ETA: 5:32 - loss: 0.2494 - acc: 0.975 - ETA: 5:27 - loss: 0.2488 - acc: 0.975 - ETA: 5:22 - loss: 0.2484 - acc: 0.975 - ETA: 5:16 - loss: 0.2480 - acc: 0.975 - ETA: 5:11 - loss: 0.2475 - acc: 0.975 - ETA: 5:06 - loss: 0.2469 - acc: 0.975 - ETA: 5:01 - loss: 0.2467 - acc: 0.975 - ETA: 4:56 - loss: 0.2464 - acc: 0.975 - ETA: 4:50 - loss: 0.2462 - acc: 0.975 - ETA: 4:45 - loss: 0.2456 - acc: 0.975 - ETA: 4:40 - loss: 0.2452 - acc: 0.975 - ETA: 4:35 - loss: 0.2446 - acc: 0.975 - ETA: 4:29 - loss: 0.2443 - acc: 0.975 - ETA: 4:24 - loss: 0.2438 - acc: 0.975 - ETA: 4:19 - loss: 0.2435 - acc: 0.975 - ETA: 4:14 - loss: 0.2433 - acc: 0.975 - ETA: 4:08 - loss: 0.2433 - acc: 0.975 - ETA: 4:03 - loss: 0.2428 - acc: 0.975 - ETA: 3:58 - loss: 0.2423 - acc: 0.975 - ETA: 3:53 - loss: 0.2419 - acc: 0.975 - ETA: 3:48 - loss: 0.2416 - acc: 0.975 - ETA: 3:42 - loss: 0.2412 - acc: 0.975 - ETA: 3:37 - loss: 0.2408 - acc: 0.975 - ETA: 3:32 - loss: 0.2403 - acc: 0.975 - ETA: 3:26 - loss: 0.2398 - acc: 0.975 - ETA: 3:21 - loss: 0.2394 - acc: 0.975 - ETA: 3:16 - loss: 0.2391 - acc: 0.975 - ETA: 3:11 - loss: 0.2389 - acc: 0.975 - ETA: 3:05 - loss: 0.2384 - acc: 0.975 - ETA: 3:00 - loss: 0.2384 - acc: 0.975 - ETA: 2:55 - loss: 0.2380 - acc: 0.975 - ETA: 2:50 - loss: 0.2379 - acc: 0.975 - ETA: 2:44 - loss: 0.2379 - acc: 0.975 - ETA: 2:39 - loss: 0.2376 - acc: 0.975 - ETA: 2:34 - loss: 0.2376 - acc: 0.975 - ETA: 2:28 - loss: 0.2374 - acc: 0.975 - ETA: 2:23 - loss: 0.2372 - acc: 0.975 - ETA: 2:18 - loss: 0.2370 - acc: 0.975 - ETA: 2:13 - loss: 0.2366 - acc: 0.975 - ETA: 2:07 - loss: 0.2360 - acc: 0.975 - ETA: 2:02 - loss: 0.2358 - acc: 0.975 - ETA: 1:57 - loss: 0.2353 - acc: 0.975 - ETA: 1:51 - loss: 0.2349 - acc: 0.975 - ETA: 1:46 - loss: 0.2348 - acc: 0.975 - ETA: 1:41 - loss: 0.2343 - acc: 0.975 - ETA: 1:35 - loss: 0.2339 - acc: 0.975 - ETA: 1:30 - loss: 0.2335 - acc: 0.975 - ETA: 1:25 - loss: 0.2334 - acc: 0.975 - ETA: 1:19 - loss: 0.2329 - acc: 0.975 - ETA: 1:14 - loss: 0.2325 - acc: 0.975 - ETA: 1:09 - loss: 0.2325 - acc: 0.975 - ETA: 1:03 - loss: 0.2322 - acc: 0.975 - ETA: 58s - loss: 0.2319 - acc: 0.975 - ETA: 53s - loss: 0.2317 - acc: 0.97 - ETA: 47s - loss: 0.2316 - acc: 0.97 - ETA: 42s - loss: 0.2315 - acc: 0.97 - ETA: 37s - loss: 0.2310 - acc: 0.97 - ETA: 31s - loss: 0.2307 - acc: 0.97 - ETA: 26s - loss: 0.2307 - acc: 0.97 - ETA: 21s - loss: 0.2307 - acc: 0.97 - ETA: 16s - loss: 0.2302 - acc: 0.97 - ETA: 10s - loss: 0.2298 - acc: 0.97 - ETA: 5s - loss: 0.2295 - acc: 0.9751 - 2169s 88ms/step - loss: 0.2290 - acc: 0.9752 - val_loss: 0.1165 - val_acc: 0.9766\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13056/24640 [==============>...............] - ETA: 36:19 - loss: 0.2425 - acc: 0.93 - ETA: 36:04 - loss: 0.1863 - acc: 0.95 - ETA: 35:50 - loss: 0.1730 - acc: 0.95 - ETA: 35:47 - loss: 0.1667 - acc: 0.96 - ETA: 35:47 - loss: 0.1817 - acc: 0.95 - ETA: 35:47 - loss: 0.1675 - acc: 0.96 - ETA: 35:41 - loss: 0.1640 - acc: 0.96 - ETA: 35:36 - loss: 0.1559 - acc: 0.96 - ETA: 35:36 - loss: 0.1582 - acc: 0.96 - ETA: 35:33 - loss: 0.1567 - acc: 0.96 - ETA: 35:29 - loss: 0.1597 - acc: 0.96 - ETA: 35:08 - loss: 0.1537 - acc: 0.96 - ETA: 35:02 - loss: 0.1492 - acc: 0.96 - ETA: 34:53 - loss: 0.1450 - acc: 0.96 - ETA: 34:47 - loss: 0.1417 - acc: 0.96 - ETA: 34:44 - loss: 0.1390 - acc: 0.96 - ETA: 34:48 - loss: 0.1421 - acc: 0.96 - ETA: 34:50 - loss: 0.1422 - acc: 0.96 - ETA: 34:45 - loss: 0.1393 - acc: 0.96 - ETA: 34:39 - loss: 0.1365 - acc: 0.97 - ETA: 34:23 - loss: 0.1364 - acc: 0.97 - ETA: 34:14 - loss: 0.1345 - acc: 0.97 - ETA: 34:06 - loss: 0.1307 - acc: 0.97 - ETA: 34:02 - loss: 0.1307 - acc: 0.97 - ETA: 33:57 - loss: 0.1272 - acc: 0.97 - ETA: 34:01 - loss: 0.1281 - acc: 0.97 - ETA: 34:03 - loss: 0.1266 - acc: 0.97 - ETA: 33:58 - loss: 0.1270 - acc: 0.97 - ETA: 33:48 - loss: 0.1242 - acc: 0.97 - ETA: 33:54 - loss: 0.1295 - acc: 0.97 - ETA: 33:46 - loss: 0.1317 - acc: 0.97 - ETA: 33:39 - loss: 0.1334 - acc: 0.97 - ETA: 33:35 - loss: 0.1307 - acc: 0.97 - ETA: 33:30 - loss: 0.1295 - acc: 0.97 - ETA: 33:25 - loss: 0.1311 - acc: 0.97 - ETA: 33:32 - loss: 0.1286 - acc: 0.97 - ETA: 33:27 - loss: 0.1304 - acc: 0.97 - ETA: 33:50 - loss: 0.1307 - acc: 0.97 - ETA: 34:07 - loss: 0.1297 - acc: 0.97 - ETA: 34:16 - loss: 0.1298 - acc: 0.97 - ETA: 34:23 - loss: 0.1276 - acc: 0.97 - ETA: 34:31 - loss: 0.1280 - acc: 0.97 - ETA: 34:32 - loss: 0.1270 - acc: 0.97 - ETA: 34:34 - loss: 0.1263 - acc: 0.97 - ETA: 34:33 - loss: 0.1256 - acc: 0.97 - ETA: 34:29 - loss: 0.1270 - acc: 0.97 - ETA: 34:28 - loss: 0.1283 - acc: 0.97 - ETA: 34:22 - loss: 0.1275 - acc: 0.97 - ETA: 34:11 - loss: 0.1266 - acc: 0.97 - ETA: 34:07 - loss: 0.1260 - acc: 0.97 - ETA: 33:57 - loss: 0.1263 - acc: 0.97 - ETA: 33:53 - loss: 0.1267 - acc: 0.97 - ETA: 33:45 - loss: 0.1267 - acc: 0.97 - ETA: 33:33 - loss: 0.1299 - acc: 0.97 - ETA: 33:20 - loss: 0.1291 - acc: 0.97 - ETA: 33:06 - loss: 0.1295 - acc: 0.97 - ETA: 32:53 - loss: 0.1298 - acc: 0.97 - ETA: 32:42 - loss: 0.1291 - acc: 0.97 - ETA: 32:29 - loss: 0.1283 - acc: 0.97 - ETA: 32:16 - loss: 0.1276 - acc: 0.97 - ETA: 32:06 - loss: 0.1269 - acc: 0.97 - ETA: 32:00 - loss: 0.1263 - acc: 0.97 - ETA: 31:49 - loss: 0.1266 - acc: 0.97 - ETA: 31:37 - loss: 0.1253 - acc: 0.97 - ETA: 31:28 - loss: 0.1255 - acc: 0.97 - ETA: 31:20 - loss: 0.1249 - acc: 0.97 - ETA: 31:12 - loss: 0.1236 - acc: 0.97 - ETA: 31:04 - loss: 0.1230 - acc: 0.97 - ETA: 30:59 - loss: 0.1225 - acc: 0.97 - ETA: 30:51 - loss: 0.1227 - acc: 0.97 - ETA: 30:45 - loss: 0.1230 - acc: 0.97 - ETA: 30:38 - loss: 0.1232 - acc: 0.97 - ETA: 30:31 - loss: 0.1242 - acc: 0.97 - ETA: 30:25 - loss: 0.1243 - acc: 0.97 - ETA: 30:16 - loss: 0.1259 - acc: 0.97 - ETA: 30:12 - loss: 0.1262 - acc: 0.97 - ETA: 30:09 - loss: 0.1263 - acc: 0.97 - ETA: 30:03 - loss: 0.1263 - acc: 0.97 - ETA: 29:55 - loss: 0.1258 - acc: 0.97 - ETA: 29:54 - loss: 0.1255 - acc: 0.97 - ETA: 29:49 - loss: 0.1250 - acc: 0.97 - ETA: 29:47 - loss: 0.1258 - acc: 0.97 - ETA: 29:42 - loss: 0.1259 - acc: 0.97 - ETA: 29:35 - loss: 0.1256 - acc: 0.97 - ETA: 29:26 - loss: 0.1259 - acc: 0.97 - ETA: 29:17 - loss: 0.1261 - acc: 0.97 - ETA: 29:12 - loss: 0.1256 - acc: 0.97 - ETA: 29:04 - loss: 0.1258 - acc: 0.97 - ETA: 28:56 - loss: 0.1248 - acc: 0.97 - ETA: 28:49 - loss: 0.1256 - acc: 0.97 - ETA: 28:41 - loss: 0.1259 - acc: 0.97 - ETA: 28:36 - loss: 0.1249 - acc: 0.97 - ETA: 28:30 - loss: 0.1261 - acc: 0.97 - ETA: 28:26 - loss: 0.1251 - acc: 0.97 - ETA: 28:26 - loss: 0.1248 - acc: 0.97 - ETA: 28:19 - loss: 0.1249 - acc: 0.97 - ETA: 28:19 - loss: 0.1246 - acc: 0.97 - ETA: 28:25 - loss: 0.1237 - acc: 0.97 - ETA: 28:21 - loss: 0.1234 - acc: 0.97 - ETA: 28:14 - loss: 0.1236 - acc: 0.97 - ETA: 28:05 - loss: 0.1249 - acc: 0.97 - ETA: 28:03 - loss: 0.1251 - acc: 0.97 - ETA: 27:57 - loss: 0.1263 - acc: 0.97 - ETA: 27:57 - loss: 0.1254 - acc: 0.97 - ETA: 27:50 - loss: 0.1251 - acc: 0.97 - ETA: 27:45 - loss: 0.1258 - acc: 0.97 - ETA: 27:37 - loss: 0.1255 - acc: 0.97 - ETA: 27:33 - loss: 0.1257 - acc: 0.97 - ETA: 27:27 - loss: 0.1267 - acc: 0.97 - ETA: 27:19 - loss: 0.1269 - acc: 0.97 - ETA: 27:12 - loss: 0.1261 - acc: 0.97 - ETA: 27:06 - loss: 0.1257 - acc: 0.97 - ETA: 26:59 - loss: 0.1259 - acc: 0.97 - ETA: 26:54 - loss: 0.1260 - acc: 0.97 - ETA: 26:52 - loss: 0.1252 - acc: 0.97 - ETA: 26:47 - loss: 0.1248 - acc: 0.97 - ETA: 26:41 - loss: 0.1240 - acc: 0.97 - ETA: 26:33 - loss: 0.1245 - acc: 0.97 - ETA: 26:25 - loss: 0.1251 - acc: 0.97 - ETA: 26:17 - loss: 0.1261 - acc: 0.97 - ETA: 26:09 - loss: 0.1258 - acc: 0.97 - ETA: 26:01 - loss: 0.1254 - acc: 0.97 - ETA: 25:53 - loss: 0.1251 - acc: 0.97 - ETA: 25:45 - loss: 0.1243 - acc: 0.97 - ETA: 25:37 - loss: 0.1241 - acc: 0.97 - ETA: 25:29 - loss: 0.1234 - acc: 0.97 - ETA: 25:22 - loss: 0.1239 - acc: 0.97 - ETA: 25:16 - loss: 0.1232 - acc: 0.97 - ETA: 25:10 - loss: 0.1229 - acc: 0.97 - ETA: 25:04 - loss: 0.1235 - acc: 0.97 - ETA: 25:00 - loss: 0.1232 - acc: 0.97 - ETA: 24:54 - loss: 0.1230 - acc: 0.97 - ETA: 24:48 - loss: 0.1232 - acc: 0.97 - ETA: 24:43 - loss: 0.1225 - acc: 0.97 - ETA: 24:37 - loss: 0.1226 - acc: 0.97 - ETA: 24:31 - loss: 0.1220 - acc: 0.97 - ETA: 24:25 - loss: 0.1220 - acc: 0.97 - ETA: 24:20 - loss: 0.1222 - acc: 0.97 - ETA: 24:14 - loss: 0.1223 - acc: 0.97 - ETA: 24:08 - loss: 0.1216 - acc: 0.97 - ETA: 24:02 - loss: 0.1210 - acc: 0.97 - ETA: 23:57 - loss: 0.1204 - acc: 0.97 - ETA: 23:53 - loss: 0.1198 - acc: 0.97 - ETA: 23:50 - loss: 0.1196 - acc: 0.97 - ETA: 23:44 - loss: 0.1190 - acc: 0.97 - ETA: 23:39 - loss: 0.1192 - acc: 0.97 - ETA: 23:33 - loss: 0.1194 - acc: 0.97 - ETA: 23:27 - loss: 0.1188 - acc: 0.97 - ETA: 23:21 - loss: 0.1182 - acc: 0.97 - ETA: 23:15 - loss: 0.1183 - acc: 0.97 - ETA: 23:11 - loss: 0.1185 - acc: 0.97 - ETA: 23:05 - loss: 0.1179 - acc: 0.97 - ETA: 23:00 - loss: 0.1173 - acc: 0.97 - ETA: 22:53 - loss: 0.1179 - acc: 0.97 - ETA: 22:48 - loss: 0.1177 - acc: 0.97 - ETA: 22:42 - loss: 0.1171 - acc: 0.97 - ETA: 22:36 - loss: 0.1170 - acc: 0.97 - ETA: 22:30 - loss: 0.1171 - acc: 0.97 - ETA: 22:24 - loss: 0.1169 - acc: 0.97 - ETA: 22:18 - loss: 0.1171 - acc: 0.97 - ETA: 22:12 - loss: 0.1169 - acc: 0.97 - ETA: 22:08 - loss: 0.1167 - acc: 0.97 - ETA: 22:03 - loss: 0.1166 - acc: 0.97 - ETA: 21:57 - loss: 0.1164 - acc: 0.97 - ETA: 21:51 - loss: 0.1173 - acc: 0.97 - ETA: 21:45 - loss: 0.1168 - acc: 0.97 - ETA: 21:39 - loss: 0.1166 - acc: 0.97 - ETA: 21:34 - loss: 0.1162 - acc: 0.97 - ETA: 21:27 - loss: 0.1157 - acc: 0.97 - ETA: 21:21 - loss: 0.1155 - acc: 0.97 - ETA: 21:15 - loss: 0.1150 - acc: 0.97 - ETA: 21:11 - loss: 0.1155 - acc: 0.97 - ETA: 21:05 - loss: 0.1162 - acc: 0.97 - ETA: 20:59 - loss: 0.1161 - acc: 0.97 - ETA: 20:54 - loss: 0.1166 - acc: 0.97 - ETA: 20:48 - loss: 0.1167 - acc: 0.97 - ETA: 20:41 - loss: 0.1162 - acc: 0.97 - ETA: 20:35 - loss: 0.1160 - acc: 0.97 - ETA: 20:28 - loss: 0.1156 - acc: 0.97 - ETA: 20:21 - loss: 0.1151 - acc: 0.97 - ETA: 20:14 - loss: 0.1149 - acc: 0.97 - ETA: 20:08 - loss: 0.1150 - acc: 0.97 - ETA: 20:02 - loss: 0.1151 - acc: 0.97 - ETA: 19:55 - loss: 0.1152 - acc: 0.97 - ETA: 19:49 - loss: 0.1154 - acc: 0.97 - ETA: 19:42 - loss: 0.1152 - acc: 0.97 - ETA: 19:36 - loss: 0.1147 - acc: 0.97 - ETA: 19:29 - loss: 0.1146 - acc: 0.97 - ETA: 19:22 - loss: 0.1141 - acc: 0.97 - ETA: 19:16 - loss: 0.1142 - acc: 0.97 - ETA: 19:09 - loss: 0.1141 - acc: 0.97 - ETA: 19:03 - loss: 0.1140 - acc: 0.97 - ETA: 18:57 - loss: 0.1148 - acc: 0.97 - ETA: 18:51 - loss: 0.1148 - acc: 0.97 - ETA: 18:45 - loss: 0.1159 - acc: 0.97 - ETA: 18:38 - loss: 0.1157 - acc: 0.97 - ETA: 18:32 - loss: 0.1153 - acc: 0.97 - ETA: 18:25 - loss: 0.1154 - acc: 0.97 - ETA: 18:19 - loss: 0.1152 - acc: 0.97 - ETA: 18:12 - loss: 0.1154 - acc: 0.97 - ETA: 18:06 - loss: 0.1155 - acc: 0.97 - ETA: 18:00 - loss: 0.1156 - acc: 0.97 - ETA: 17:54 - loss: 0.1155 - acc: 0.97 - ETA: 17:47 - loss: 0.1151 - acc: 0.9764"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24640/24640 [==============================] - ETA: 17:41 - loss: 0.1152 - acc: 0.97 - ETA: 17:35 - loss: 0.1148 - acc: 0.97 - ETA: 17:29 - loss: 0.1152 - acc: 0.97 - ETA: 17:23 - loss: 0.1153 - acc: 0.97 - ETA: 17:16 - loss: 0.1157 - acc: 0.97 - ETA: 17:10 - loss: 0.1161 - acc: 0.97 - ETA: 17:04 - loss: 0.1164 - acc: 0.97 - ETA: 16:57 - loss: 0.1165 - acc: 0.97 - ETA: 16:51 - loss: 0.1167 - acc: 0.97 - ETA: 16:45 - loss: 0.1165 - acc: 0.97 - ETA: 16:38 - loss: 0.1169 - acc: 0.97 - ETA: 16:33 - loss: 0.1173 - acc: 0.97 - ETA: 16:27 - loss: 0.1171 - acc: 0.97 - ETA: 16:21 - loss: 0.1170 - acc: 0.97 - ETA: 16:15 - loss: 0.1176 - acc: 0.97 - ETA: 16:09 - loss: 0.1174 - acc: 0.97 - ETA: 16:02 - loss: 0.1173 - acc: 0.97 - ETA: 15:56 - loss: 0.1169 - acc: 0.97 - ETA: 15:50 - loss: 0.1168 - acc: 0.97 - ETA: 15:44 - loss: 0.1171 - acc: 0.97 - ETA: 15:38 - loss: 0.1172 - acc: 0.97 - ETA: 15:32 - loss: 0.1172 - acc: 0.97 - ETA: 15:26 - loss: 0.1171 - acc: 0.97 - ETA: 15:21 - loss: 0.1177 - acc: 0.97 - ETA: 15:15 - loss: 0.1178 - acc: 0.97 - ETA: 15:09 - loss: 0.1176 - acc: 0.97 - ETA: 15:03 - loss: 0.1175 - acc: 0.97 - ETA: 14:57 - loss: 0.1173 - acc: 0.97 - ETA: 14:51 - loss: 0.1170 - acc: 0.97 - ETA: 14:45 - loss: 0.1166 - acc: 0.97 - ETA: 14:39 - loss: 0.1164 - acc: 0.97 - ETA: 14:33 - loss: 0.1161 - acc: 0.97 - ETA: 14:27 - loss: 0.1160 - acc: 0.97 - ETA: 14:22 - loss: 0.1165 - acc: 0.97 - ETA: 14:16 - loss: 0.1167 - acc: 0.97 - ETA: 14:10 - loss: 0.1166 - acc: 0.97 - ETA: 14:04 - loss: 0.1162 - acc: 0.97 - ETA: 13:58 - loss: 0.1159 - acc: 0.97 - ETA: 13:52 - loss: 0.1161 - acc: 0.97 - ETA: 13:46 - loss: 0.1162 - acc: 0.97 - ETA: 13:40 - loss: 0.1163 - acc: 0.97 - ETA: 13:34 - loss: 0.1170 - acc: 0.97 - ETA: 13:28 - loss: 0.1173 - acc: 0.97 - ETA: 13:23 - loss: 0.1170 - acc: 0.97 - ETA: 13:17 - loss: 0.1168 - acc: 0.97 - ETA: 13:11 - loss: 0.1172 - acc: 0.97 - ETA: 13:05 - loss: 0.1170 - acc: 0.97 - ETA: 13:00 - loss: 0.1172 - acc: 0.97 - ETA: 12:54 - loss: 0.1168 - acc: 0.97 - ETA: 12:48 - loss: 0.1167 - acc: 0.97 - ETA: 12:42 - loss: 0.1168 - acc: 0.97 - ETA: 12:36 - loss: 0.1169 - acc: 0.97 - ETA: 12:30 - loss: 0.1170 - acc: 0.97 - ETA: 12:24 - loss: 0.1171 - acc: 0.97 - ETA: 12:18 - loss: 0.1170 - acc: 0.97 - ETA: 12:13 - loss: 0.1170 - acc: 0.97 - ETA: 12:07 - loss: 0.1171 - acc: 0.97 - ETA: 12:01 - loss: 0.1174 - acc: 0.97 - ETA: 11:55 - loss: 0.1177 - acc: 0.97 - ETA: 11:49 - loss: 0.1176 - acc: 0.97 - ETA: 11:43 - loss: 0.1177 - acc: 0.97 - ETA: 11:37 - loss: 0.1174 - acc: 0.97 - ETA: 11:31 - loss: 0.1179 - acc: 0.97 - ETA: 11:25 - loss: 0.1180 - acc: 0.97 - ETA: 11:19 - loss: 0.1181 - acc: 0.97 - ETA: 11:14 - loss: 0.1178 - acc: 0.97 - ETA: 11:08 - loss: 0.1179 - acc: 0.97 - ETA: 11:02 - loss: 0.1179 - acc: 0.97 - ETA: 10:56 - loss: 0.1182 - acc: 0.97 - ETA: 10:50 - loss: 0.1183 - acc: 0.97 - ETA: 10:44 - loss: 0.1182 - acc: 0.97 - ETA: 10:38 - loss: 0.1181 - acc: 0.97 - ETA: 10:32 - loss: 0.1183 - acc: 0.97 - ETA: 10:27 - loss: 0.1180 - acc: 0.97 - ETA: 10:21 - loss: 0.1177 - acc: 0.97 - ETA: 10:15 - loss: 0.1178 - acc: 0.97 - ETA: 10:10 - loss: 0.1177 - acc: 0.97 - ETA: 10:04 - loss: 0.1178 - acc: 0.97 - ETA: 9:58 - loss: 0.1177 - acc: 0.9755 - ETA: 9:52 - loss: 0.1175 - acc: 0.975 - ETA: 9:46 - loss: 0.1177 - acc: 0.975 - ETA: 9:40 - loss: 0.1175 - acc: 0.975 - ETA: 9:34 - loss: 0.1174 - acc: 0.975 - ETA: 9:29 - loss: 0.1176 - acc: 0.975 - ETA: 9:23 - loss: 0.1178 - acc: 0.975 - ETA: 9:17 - loss: 0.1177 - acc: 0.975 - ETA: 9:12 - loss: 0.1178 - acc: 0.975 - ETA: 9:06 - loss: 0.1178 - acc: 0.975 - ETA: 9:01 - loss: 0.1175 - acc: 0.975 - ETA: 8:55 - loss: 0.1176 - acc: 0.975 - ETA: 8:49 - loss: 0.1173 - acc: 0.975 - ETA: 8:44 - loss: 0.1170 - acc: 0.975 - ETA: 8:38 - loss: 0.1173 - acc: 0.975 - ETA: 8:32 - loss: 0.1172 - acc: 0.975 - ETA: 8:26 - loss: 0.1169 - acc: 0.975 - ETA: 8:20 - loss: 0.1170 - acc: 0.975 - ETA: 8:14 - loss: 0.1169 - acc: 0.975 - ETA: 8:09 - loss: 0.1169 - acc: 0.975 - ETA: 8:03 - loss: 0.1172 - acc: 0.975 - ETA: 7:57 - loss: 0.1173 - acc: 0.975 - ETA: 7:51 - loss: 0.1172 - acc: 0.975 - ETA: 7:46 - loss: 0.1169 - acc: 0.975 - ETA: 7:39 - loss: 0.1168 - acc: 0.975 - ETA: 2:58:06 - loss: 0.1169 - acc: 0.97 - ETA: 2:55:15 - loss: 0.1171 - acc: 0.97 - ETA: 2:52:25 - loss: 0.1172 - acc: 0.97 - ETA: 2:49:35 - loss: 0.1174 - acc: 0.97 - ETA: 2:46:47 - loss: 0.1175 - acc: 0.97 - ETA: 2:44:00 - loss: 0.1176 - acc: 0.97 - ETA: 2:41:13 - loss: 0.1180 - acc: 0.97 - ETA: 2:38:28 - loss: 0.1178 - acc: 0.97 - ETA: 2:35:43 - loss: 0.1180 - acc: 0.97 - ETA: 2:33:00 - loss: 0.1183 - acc: 0.97 - ETA: 2:30:17 - loss: 0.1180 - acc: 0.97 - ETA: 2:27:36 - loss: 0.1181 - acc: 0.97 - ETA: 2:24:56 - loss: 0.1180 - acc: 0.97 - ETA: 2:22:16 - loss: 0.1177 - acc: 0.97 - ETA: 2:19:37 - loss: 0.1179 - acc: 0.97 - ETA: 2:17:00 - loss: 0.1180 - acc: 0.97 - ETA: 2:14:23 - loss: 0.1181 - acc: 0.97 - ETA: 2:11:47 - loss: 0.1184 - acc: 0.97 - ETA: 2:09:12 - loss: 0.1183 - acc: 0.97 - ETA: 2:06:38 - loss: 0.1182 - acc: 0.97 - ETA: 2:04:05 - loss: 0.1181 - acc: 0.97 - ETA: 2:01:34 - loss: 0.1182 - acc: 0.97 - ETA: 1:59:03 - loss: 0.1184 - acc: 0.97 - ETA: 1:56:32 - loss: 0.1188 - acc: 0.97 - ETA: 1:54:03 - loss: 0.1188 - acc: 0.97 - ETA: 1:51:34 - loss: 0.1188 - acc: 0.97 - ETA: 1:49:07 - loss: 0.1186 - acc: 0.97 - ETA: 1:46:40 - loss: 0.1187 - acc: 0.97 - ETA: 1:44:14 - loss: 0.1188 - acc: 0.97 - ETA: 1:41:49 - loss: 0.1189 - acc: 0.97 - ETA: 1:39:24 - loss: 0.1189 - acc: 0.97 - ETA: 1:37:01 - loss: 0.1188 - acc: 0.97 - ETA: 1:34:38 - loss: 0.1189 - acc: 0.97 - ETA: 1:32:16 - loss: 0.1189 - acc: 0.97 - ETA: 1:29:55 - loss: 0.1189 - acc: 0.97 - ETA: 1:27:35 - loss: 0.1186 - acc: 0.97 - ETA: 1:25:16 - loss: 0.1187 - acc: 0.97 - ETA: 1:22:57 - loss: 0.1189 - acc: 0.97 - ETA: 1:20:39 - loss: 0.1190 - acc: 0.97 - ETA: 1:18:22 - loss: 0.1187 - acc: 0.97 - ETA: 1:16:06 - loss: 0.1188 - acc: 0.97 - ETA: 1:13:50 - loss: 0.1187 - acc: 0.97 - ETA: 1:11:35 - loss: 0.1186 - acc: 0.97 - ETA: 1:09:21 - loss: 0.1187 - acc: 0.97 - ETA: 1:07:08 - loss: 0.1185 - acc: 0.97 - ETA: 1:04:55 - loss: 0.1184 - acc: 0.97 - ETA: 1:02:43 - loss: 0.1182 - acc: 0.97 - ETA: 1:00:32 - loss: 0.1181 - acc: 0.97 - ETA: 58:21 - loss: 0.1182 - acc: 0.9754 - ETA: 56:11 - loss: 0.1186 - acc: 0.97 - ETA: 54:02 - loss: 0.1185 - acc: 0.97 - ETA: 51:54 - loss: 0.1182 - acc: 0.97 - ETA: 49:46 - loss: 0.1183 - acc: 0.97 - ETA: 47:39 - loss: 0.1180 - acc: 0.97 - ETA: 45:33 - loss: 0.1182 - acc: 0.97 - ETA: 43:27 - loss: 0.1185 - acc: 0.97 - ETA: 41:22 - loss: 0.1184 - acc: 0.97 - ETA: 39:18 - loss: 0.1185 - acc: 0.97 - ETA: 37:14 - loss: 0.1185 - acc: 0.97 - ETA: 35:11 - loss: 0.1184 - acc: 0.97 - ETA: 33:08 - loss: 0.1183 - acc: 0.97 - ETA: 31:06 - loss: 0.1186 - acc: 0.97 - ETA: 29:05 - loss: 0.1186 - acc: 0.97 - ETA: 27:04 - loss: 0.1187 - acc: 0.97 - ETA: 25:05 - loss: 0.1192 - acc: 0.97 - ETA: 23:05 - loss: 0.1194 - acc: 0.97 - ETA: 21:06 - loss: 0.1195 - acc: 0.97 - ETA: 19:08 - loss: 0.1193 - acc: 0.97 - ETA: 17:11 - loss: 0.1192 - acc: 0.97 - ETA: 15:14 - loss: 0.1191 - acc: 0.97 - ETA: 13:17 - loss: 0.1189 - acc: 0.97 - ETA: 11:22 - loss: 0.1186 - acc: 0.97 - ETA: 9:27 - loss: 0.1190 - acc: 0.9752 - ETA: 7:32 - loss: 0.1189 - acc: 0.975 - ETA: 5:38 - loss: 0.1190 - acc: 0.975 - ETA: 3:45 - loss: 0.1190 - acc: 0.975 - ETA: 1:52 - loss: 0.1188 - acc: 0.975 - 43200s 2s/step - loss: 0.1188 - acc: 0.9752 - val_loss: 0.1105 - val_acc: 0.9766\n",
      "6161/6161 [==============================] - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 81s 13ms/step\n",
      "Test loss (LOWER is better)      : 0.1104510285016503\n",
      "Test accuracy (HIGHER is better) : 0.9766271709138127\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit(x_train, y_train, batch_size=64, epochs=2,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "loss, acc = lstm_model.evaluate(x_test, y_test, batch_size=64)\n",
    "\n",
    "print('Test loss (LOWER is better)      :', loss)\n",
    "print('Test accuracy (HIGHER is better) :', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = lstm_model.predict_classes(sequences_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ACCURACY: 0.9766271709138127\n",
      "LSTM CLASSIFIER CONFUSTION MATRIX: [[6017    0]\n",
      " [ 144    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madhan.s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> **LSTM F1 SCORE [0.9881754 0.       ]\n",
      "===> **LSTM Recall SCORE [1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madhan.s\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> **LSTM Precision SCORE [0.97662717 0.        ]\n",
      "===> **LSTM ROC_AUC SCORE 0.5\n",
      "#######################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM ACCURACY:\",metrics.accuracy_score(y_test,lstm_pred))\n",
    "print('LSTM CLASSIFIER CONFUSTION MATRIX:',metrics.confusion_matrix(y_test,lstm_pred))\n",
    "print(\"===> **LSTM F1 SCORE\",metrics.f1_score(y_test, lstm_pred,average=None))\n",
    "print(\"===> **LSTM Recall SCORE\",metrics.recall_score(y_test, lstm_pred,average=None))\n",
    "print(\"===> **LSTM Precision SCORE\",metrics.precision_score(y_test, lstm_pred,average=None))\n",
    "print(\"===> **LSTM ROC_AUC SCORE\",metrics.roc_auc_score(y_test, lstm_pred,average=None))\n",
    "print(\"#######################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'C:\\Madhan\\Analytics\\Machine_Learning_project_work\\AI_ML\\bluepencil\\model_improvement\\test_set_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>fileContent</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>13th Directive \\r\\r\\nEU VAT refund \\r\\r\\nreque...</td>\n",
       "      <td>Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>Country\\r\\r\\nAustria \\r\\r\\nBelgium\\r\\r\\nBulgar...</td>\n",
       "      <td>Non-Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>2\\r\\r\\n\\r\\r\\n|   13th Directive EU VAT refund ...</td>\n",
       "      <td>Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>A company established in Hong Kong, Taiwan, Ma...</td>\n",
       "      <td>Non-Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-01-0290_Folder Tax_Vat_2017_Internet.pdf</td>\n",
       "      <td>In particular, \\r\\r\\nthe Asian company must:\\r...</td>\n",
       "      <td>Non-Legal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   filename  \\\n",
       "0           0  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "1           1  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "2           2  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "3           3  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "4           4  -01-0290_Folder Tax_Vat_2017_Internet.pdf   \n",
       "\n",
       "                                         fileContent      Label  \n",
       "0  13th Directive \\r\\r\\nEU VAT refund \\r\\r\\nreque...      Legal  \n",
       "1  Country\\r\\r\\nAustria \\r\\r\\nBelgium\\r\\r\\nBulgar...  Non-Legal  \n",
       "2  2\\r\\r\\n\\r\\r\\n|   13th Directive EU VAT refund ...      Legal  \n",
       "3  A company established in Hong Kong, Taiwan, Ma...  Non-Legal  \n",
       "4  In particular, \\r\\r\\nthe Asian company must:\\r...  Non-Legal  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test.fileContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Parsed......................\n",
      "\n",
      "Wall time: 0 ns\n",
      "Cleaned Short Descriptions 5000 of 39055\n",
      "Cleaned Short Descriptions 10000 of 39055\n",
      "Cleaned Short Descriptions 15000 of 39055\n",
      "Cleaned Short Descriptions 20000 of 39055\n",
      "Cleaned Short Descriptions 25000 of 39055\n",
      "Cleaned Short Descriptions 30000 of 39055\n",
      "Cleaned Short Descriptions 35000 of 39055\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows Parsed......................\\n\")\n",
    "corpus = []\n",
    "num_SD = len(X)\n",
    "%time\n",
    "for i in range( 0, num_SD ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Cleaned Short Descriptions %d of %d\" % ( i+1, num_SD ))\n",
    "    corpus.append( review_to_words(str(X.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dtm_test = tfidf_vec.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39055, 255)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_randforest = rand_classifer.predict(X_dtm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_randforest = pd.DataFrame(y_pred_randforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = pd.concat([test,y_pred_randforest],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1.to_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266      despite instance runaway success rather compet...\n",
       "2905          thing people work tax tend rely gut instinct\n",
       "9425     simplification investment bank tangle estate a...\n",
       "29280    majority key fund structure decision make basi...\n",
       "15696    firm across industry already take step prepare...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despite instance runaway success rather compete directly see fintechs bank exist symbiotically create strong financial system',\n",
       " 'thing people work tax tend rely gut instinct',\n",
       " 'simplification investment bank tangle estate arguably difficult cost address also one struggle tackle',\n",
       " 'majority key fund structure decision make basis two critical commercial consideration directly affect tax position would apply fund investor fund management team track record fund management team type geographic location prospective investor']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def corpus_vector(word2vec_model, corpus):\n",
    "    new_corpus=[]\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for i in range(0,50):\n",
    "            if i < len(doc) and doc[i] in word2vec_model.vocab:\n",
    "                new_doc.append(word2vec_model[doc[i]])\n",
    "            else:\n",
    "                new_doc.append([0]*300)\n",
    "        new_corpus.append(new_doc)\n",
    "    # remove out-of-vocabulary words\n",
    "    #doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    #print(doc)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 13:59:31.881639\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.zeros((1555,50,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555, 50, 768)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows Parsed......................\n",
      "\n",
      "2019-07-30 11:38:51.935663\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-528-aad0ea405691>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moov_way\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print(result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbert_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bert_embedding\\bert.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sentences, oov_way)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moov_way\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moov_way\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bert_embedding\\bert.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(self, sentences, oov_way)\u001b[0m\n\u001b[0;32m    136\u001b[0m                                          valid_length.astype(self.dtype))\n\u001b[0;32m    137\u001b[0m             for token_id, sequence_output in zip(token_ids.asnumpy(),\n\u001b[1;32m--> 138\u001b[1;33m                                                  sequence_outputs.asnumpy()):\n\u001b[0m\u001b[0;32m    139\u001b[0m                 \u001b[0mbatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moov_way\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1980\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1981\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows Parsed......................\\n\")\n",
    "bert_embed = []\n",
    "num_SD = len(sentences)\n",
    "print(datetime.now())\n",
    "for i in range( 0, num_SD):\n",
    "    doc = nltk.sent_tokenize(sentences[i])\n",
    "    #print(doc)\n",
    "    result = bert_embedding(doc,oov_way='avg')\n",
    "    #print(result)\n",
    "    bert_array = np.array(result[0][1])\n",
    "        #print(bert_array.shape)\n",
    "    bert_embed.append(bert_array)\n",
    "    np.append(array[i],bert_array)\n",
    "        #print(len(bert_embed))\n",
    "    if( (i+1)%250 == 0 ):\n",
    "        print(\"Cleaned Short Descriptions %d of %d\" % ( i+1, num_SD ))\n",
    "        print(i)\n",
    "        print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9343181 ,  0.3573656 ,  0.25016475, ..., -0.36225465,\n",
       "         0.49115077,  0.01891416],\n",
       "       [ 0.57757705, -0.11281667,  0.35245678, ..., -0.19790722,\n",
       "         0.26295584,  0.07346324],\n",
       "       [ 0.72832364,  0.30656624,  0.6930689 , ..., -0.30307057,\n",
       "         0.13134867, -0.08489612],\n",
       "       ...,\n",
       "       [-0.19381317,  0.26194164,  0.2444029 , ..., -0.20700751,\n",
       "        -0.31119695, -0.9367102 ],\n",
       "       [-0.7129689 , -0.07343789,  0.5752077 , ..., -0.13918841,\n",
       "        -0.34515035,  0.4092333 ],\n",
       "       [-0.09110375,  0.35341305,  0.12333003, ...,  0.25723186,\n",
       "        -0.32090333, -0.08453374]], dtype=float32)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(23, 768)\n",
      "(15, 768)\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "for b in bert_embed[0:5]:\n",
    "    print(b.shape)\n",
    "    arr.append(b)\n",
    "    #np.append(array,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = []\n",
    "for a in arr[0]:\n",
    "    emb.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 768)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59724288,)"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59724288,)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59769600,)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 768)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embed = np.array(bert_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555, 1, 768)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[1,768])\n",
    "    #layer = Embedding(1000,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(inputs)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = [1,768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1, 768)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                213248    \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 230,145\n",
      "Trainable params: 230,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1244 samples, validate on 311 samples\n",
      "Epoch 1/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.6581 - acc: 0.687 - ETA: 0s - loss: 0.4903 - acc: 0.770 - ETA: 0s - loss: 0.4719 - acc: 0.771 - ETA: 0s - loss: 0.4796 - acc: 0.755 - ETA: 0s - loss: 0.4796 - acc: 0.753 - ETA: 0s - loss: 0.4834 - acc: 0.752 - ETA: 0s - loss: 0.4832 - acc: 0.759 - ETA: 0s - loss: 0.4848 - acc: 0.763 - 0s 384us/step - loss: 0.4818 - acc: 0.7629 - val_loss: 1.4059 - val_acc: 0.2605\n",
      "Epoch 2/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.4784 - acc: 0.750 - ETA: 0s - loss: 0.4569 - acc: 0.775 - ETA: 0s - loss: 0.4386 - acc: 0.795 - ETA: 0s - loss: 0.4391 - acc: 0.796 - ETA: 0s - loss: 0.4222 - acc: 0.809 - ETA: 0s - loss: 0.4242 - acc: 0.807 - ETA: 0s - loss: 0.4302 - acc: 0.803 - ETA: 0s - loss: 0.4379 - acc: 0.799 - ETA: 0s - loss: 0.4434 - acc: 0.797 - 1s 433us/step - loss: 0.4512 - acc: 0.7934 - val_loss: 1.5943 - val_acc: 0.1833\n",
      "Epoch 3/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.5155 - acc: 0.687 - ETA: 0s - loss: 0.4479 - acc: 0.767 - ETA: 0s - loss: 0.4631 - acc: 0.750 - ETA: 0s - loss: 0.4536 - acc: 0.759 - ETA: 0s - loss: 0.4451 - acc: 0.770 - ETA: 0s - loss: 0.4346 - acc: 0.780 - ETA: 0s - loss: 0.4498 - acc: 0.768 - ETA: 0s - loss: 0.4514 - acc: 0.767 - 1s 402us/step - loss: 0.4461 - acc: 0.7757 - val_loss: 0.9971 - val_acc: 0.4887\n",
      "Epoch 4/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3955 - acc: 0.843 - ETA: 0s - loss: 0.4166 - acc: 0.849 - ETA: 0s - loss: 0.4154 - acc: 0.828 - ETA: 0s - loss: 0.4267 - acc: 0.816 - ETA: 0s - loss: 0.4208 - acc: 0.812 - ETA: 0s - loss: 0.4172 - acc: 0.808 - ETA: 0s - loss: 0.4397 - acc: 0.795 - ETA: 0s - loss: 0.4440 - acc: 0.789 - ETA: 0s - loss: 0.4436 - acc: 0.788 - 1s 404us/step - loss: 0.4425 - acc: 0.7894 - val_loss: 1.6072 - val_acc: 0.2090\n",
      "Epoch 5/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3582 - acc: 0.843 - ETA: 0s - loss: 0.4101 - acc: 0.825 - ETA: 0s - loss: 0.4087 - acc: 0.815 - ETA: 0s - loss: 0.3986 - acc: 0.817 - ETA: 0s - loss: 0.4032 - acc: 0.822 - ETA: 0s - loss: 0.4104 - acc: 0.808 - ETA: 0s - loss: 0.4158 - acc: 0.802 - ETA: 0s - loss: 0.4166 - acc: 0.798 - ETA: 0s - loss: 0.4232 - acc: 0.790 - 1s 414us/step - loss: 0.4246 - acc: 0.7870 - val_loss: 1.3325 - val_acc: 0.3215\n",
      "Epoch 6/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3231 - acc: 0.968 - ETA: 0s - loss: 0.3872 - acc: 0.812 - ETA: 0s - loss: 0.3676 - acc: 0.823 - ETA: 0s - loss: 0.4075 - acc: 0.800 - ETA: 0s - loss: 0.4052 - acc: 0.809 - ETA: 0s - loss: 0.4098 - acc: 0.802 - ETA: 0s - loss: 0.4084 - acc: 0.804 - ETA: 0s - loss: 0.4130 - acc: 0.801 - 0s 398us/step - loss: 0.4150 - acc: 0.7966 - val_loss: 2.0947 - val_acc: 0.1576\n",
      "Epoch 7/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3710 - acc: 0.843 - ETA: 0s - loss: 0.4000 - acc: 0.818 - ETA: 0s - loss: 0.4338 - acc: 0.787 - ETA: 0s - loss: 0.4128 - acc: 0.791 - ETA: 0s - loss: 0.4194 - acc: 0.784 - ETA: 0s - loss: 0.4116 - acc: 0.791 - ETA: 0s - loss: 0.4061 - acc: 0.796 - ETA: 0s - loss: 0.3999 - acc: 0.802 - ETA: 0s - loss: 0.3979 - acc: 0.797 - 1s 406us/step - loss: 0.4026 - acc: 0.7966 - val_loss: 1.5370 - val_acc: 0.3280\n",
      "Epoch 8/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.1672 - acc: 0.968 - ETA: 0s - loss: 0.3737 - acc: 0.791 - ETA: 0s - loss: 0.4075 - acc: 0.778 - ETA: 0s - loss: 0.3956 - acc: 0.785 - ETA: 0s - loss: 0.3886 - acc: 0.792 - ETA: 0s - loss: 0.3854 - acc: 0.802 - ETA: 0s - loss: 0.3853 - acc: 0.809 - ETA: 0s - loss: 0.3836 - acc: 0.810 - ETA: 0s - loss: 0.4006 - acc: 0.802 - 1s 420us/step - loss: 0.3982 - acc: 0.8023 - val_loss: 1.1805 - val_acc: 0.4695\n",
      "Epoch 9/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3583 - acc: 0.812 - ETA: 0s - loss: 0.3426 - acc: 0.807 - ETA: 0s - loss: 0.3631 - acc: 0.803 - ETA: 0s - loss: 0.3840 - acc: 0.785 - ETA: 0s - loss: 0.3964 - acc: 0.782 - ETA: 0s - loss: 0.3904 - acc: 0.791 - ETA: 0s - loss: 0.3950 - acc: 0.794 - ETA: 0s - loss: 0.3856 - acc: 0.800 - ETA: 0s - loss: 0.3882 - acc: 0.801 - 1s 406us/step - loss: 0.3891 - acc: 0.8006 - val_loss: 1.2305 - val_acc: 0.4662\n",
      "Epoch 10/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3054 - acc: 0.875 - ETA: 0s - loss: 0.3152 - acc: 0.870 - ETA: 0s - loss: 0.3120 - acc: 0.865 - ETA: 0s - loss: 0.3490 - acc: 0.843 - ETA: 0s - loss: 0.3491 - acc: 0.842 - ETA: 0s - loss: 0.3595 - acc: 0.837 - ETA: 0s - loss: 0.3633 - acc: 0.831 - ETA: 0s - loss: 0.3784 - acc: 0.823 - ETA: 0s - loss: 0.3773 - acc: 0.821 - 1s 435us/step - loss: 0.3766 - acc: 0.8215 - val_loss: 1.3221 - val_acc: 0.4180\n",
      "Epoch 11/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3324 - acc: 0.875 - ETA: 0s - loss: 0.3225 - acc: 0.828 - ETA: 0s - loss: 0.3283 - acc: 0.831 - ETA: 0s - loss: 0.3376 - acc: 0.830 - ETA: 0s - loss: 0.3372 - acc: 0.822 - ETA: 0s - loss: 0.3532 - acc: 0.816 - ETA: 0s - loss: 0.3574 - acc: 0.808 - ETA: 0s - loss: 0.3612 - acc: 0.807 - ETA: 0s - loss: 0.3682 - acc: 0.804 - 1s 408us/step - loss: 0.3658 - acc: 0.8071 - val_loss: 1.6620 - val_acc: 0.3537\n",
      "Epoch 12/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3057 - acc: 0.812 - ETA: 0s - loss: 0.3210 - acc: 0.859 - ETA: 0s - loss: 0.3728 - acc: 0.809 - ETA: 0s - loss: 0.3616 - acc: 0.821 - ETA: 0s - loss: 0.3667 - acc: 0.814 - ETA: 0s - loss: 0.3673 - acc: 0.815 - ETA: 0s - loss: 0.3641 - acc: 0.815 - ETA: 0s - loss: 0.3603 - acc: 0.818 - ETA: 0s - loss: 0.3571 - acc: 0.818 - 1s 414us/step - loss: 0.3614 - acc: 0.8159 - val_loss: 1.4761 - val_acc: 0.4019\n",
      "Epoch 13/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.4227 - acc: 0.781 - ETA: 0s - loss: 0.3299 - acc: 0.843 - ETA: 0s - loss: 0.3600 - acc: 0.821 - ETA: 0s - loss: 0.3415 - acc: 0.830 - ETA: 0s - loss: 0.3371 - acc: 0.827 - ETA: 0s - loss: 0.3363 - acc: 0.829 - ETA: 0s - loss: 0.3427 - acc: 0.826 - ETA: 0s - loss: 0.3570 - acc: 0.814 - 0s 371us/step - loss: 0.3581 - acc: 0.8127 - val_loss: 1.9917 - val_acc: 0.2797\n",
      "Epoch 14/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2792 - acc: 0.781 - ETA: 0s - loss: 0.3342 - acc: 0.812 - ETA: 0s - loss: 0.3433 - acc: 0.809 - ETA: 0s - loss: 0.3349 - acc: 0.814 - ETA: 0s - loss: 0.3494 - acc: 0.804 - ETA: 0s - loss: 0.3382 - acc: 0.816 - ETA: 0s - loss: 0.3520 - acc: 0.808 - ETA: 0s - loss: 0.3624 - acc: 0.804 - ETA: 0s - loss: 0.3615 - acc: 0.806 - 1s 407us/step - loss: 0.3600 - acc: 0.8087 - val_loss: 1.9446 - val_acc: 0.3055\n",
      "Epoch 15/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.4149 - acc: 0.812 - ETA: 0s - loss: 0.3614 - acc: 0.791 - ETA: 0s - loss: 0.3382 - acc: 0.815 - ETA: 0s - loss: 0.3294 - acc: 0.816 - ETA: 0s - loss: 0.3400 - acc: 0.809 - ETA: 0s - loss: 0.3416 - acc: 0.810 - ETA: 0s - loss: 0.3383 - acc: 0.812 - ETA: 0s - loss: 0.3472 - acc: 0.808 - 0s 372us/step - loss: 0.3543 - acc: 0.8055 - val_loss: 1.6217 - val_acc: 0.3344\n",
      "Epoch 16/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.4599 - acc: 0.718 - ETA: 0s - loss: 0.3325 - acc: 0.825 - ETA: 0s - loss: 0.3040 - acc: 0.847 - ETA: 0s - loss: 0.3112 - acc: 0.838 - ETA: 0s - loss: 0.3216 - acc: 0.829 - ETA: 0s - loss: 0.3274 - acc: 0.825 - ETA: 0s - loss: 0.3219 - acc: 0.831 - ETA: 0s - loss: 0.3211 - acc: 0.834 - ETA: 0s - loss: 0.3271 - acc: 0.827 - 1s 419us/step - loss: 0.3419 - acc: 0.8207 - val_loss: 2.1135 - val_acc: 0.2283\n",
      "Epoch 17/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2553 - acc: 0.875 - ETA: 0s - loss: 0.3154 - acc: 0.817 - ETA: 0s - loss: 0.3264 - acc: 0.806 - ETA: 0s - loss: 0.3150 - acc: 0.824 - ETA: 0s - loss: 0.3212 - acc: 0.818 - ETA: 0s - loss: 0.3268 - acc: 0.811 - ETA: 0s - loss: 0.3257 - acc: 0.813 - ETA: 0s - loss: 0.3293 - acc: 0.815 - 0s 364us/step - loss: 0.3336 - acc: 0.8127 - val_loss: 2.9654 - val_acc: 0.1768\n",
      "Epoch 18/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.5354 - acc: 0.718 - ETA: 0s - loss: 0.3432 - acc: 0.799 - ETA: 0s - loss: 0.3275 - acc: 0.818 - ETA: 0s - loss: 0.3353 - acc: 0.814 - ETA: 0s - loss: 0.3521 - acc: 0.801 - ETA: 0s - loss: 0.3540 - acc: 0.796 - ETA: 0s - loss: 0.3460 - acc: 0.802 - ETA: 0s - loss: 0.3450 - acc: 0.801 - ETA: 0s - loss: 0.3398 - acc: 0.802 - 1s 403us/step - loss: 0.3432 - acc: 0.8006 - val_loss: 2.8195 - val_acc: 0.2058\n",
      "Epoch 19/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2701 - acc: 0.812 - ETA: 0s - loss: 0.3257 - acc: 0.828 - ETA: 0s - loss: 0.3253 - acc: 0.823 - ETA: 0s - loss: 0.3103 - acc: 0.834 - ETA: 0s - loss: 0.3319 - acc: 0.819 - ETA: 0s - loss: 0.3360 - acc: 0.812 - ETA: 0s - loss: 0.3364 - acc: 0.813 - ETA: 0s - loss: 0.3375 - acc: 0.810 - 0s 363us/step - loss: 0.3365 - acc: 0.8111 - val_loss: 2.6063 - val_acc: 0.2154\n",
      "Epoch 20/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2282 - acc: 0.906 - ETA: 0s - loss: 0.2863 - acc: 0.856 - ETA: 0s - loss: 0.3060 - acc: 0.820 - ETA: 0s - loss: 0.3178 - acc: 0.822 - ETA: 0s - loss: 0.3119 - acc: 0.834 - ETA: 0s - loss: 0.3001 - acc: 0.836 - ETA: 0s - loss: 0.3140 - acc: 0.830 - ETA: 0s - loss: 0.3260 - acc: 0.819 - ETA: 0s - loss: 0.3372 - acc: 0.814 - 1s 416us/step - loss: 0.3342 - acc: 0.8159 - val_loss: 2.2987 - val_acc: 0.2765\n",
      "Epoch 21/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2612 - acc: 0.906 - ETA: 0s - loss: 0.2823 - acc: 0.875 - ETA: 0s - loss: 0.2737 - acc: 0.865 - ETA: 0s - loss: 0.2798 - acc: 0.860 - ETA: 0s - loss: 0.3030 - acc: 0.845 - ETA: 0s - loss: 0.3063 - acc: 0.842 - ETA: 0s - loss: 0.3280 - acc: 0.834 - ETA: 0s - loss: 0.3265 - acc: 0.832 - ETA: 0s - loss: 0.3295 - acc: 0.826 - 0s 390us/step - loss: 0.3300 - acc: 0.8256 - val_loss: 2.1792 - val_acc: 0.2733\n",
      "Epoch 22/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.4379 - acc: 0.750 - ETA: 0s - loss: 0.3088 - acc: 0.828 - ETA: 0s - loss: 0.2942 - acc: 0.852 - ETA: 0s - loss: 0.3225 - acc: 0.838 - ETA: 0s - loss: 0.3211 - acc: 0.837 - ETA: 0s - loss: 0.3004 - acc: 0.850 - ETA: 0s - loss: 0.3065 - acc: 0.841 - ETA: 0s - loss: 0.3186 - acc: 0.836 - ETA: 0s - loss: 0.3231 - acc: 0.831 - 1s 413us/step - loss: 0.3192 - acc: 0.8328 - val_loss: 2.7486 - val_acc: 0.2315\n",
      "Epoch 23/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2704 - acc: 0.812 - ETA: 0s - loss: 0.2955 - acc: 0.839 - ETA: 0s - loss: 0.3147 - acc: 0.835 - ETA: 0s - loss: 0.3120 - acc: 0.834 - ETA: 0s - loss: 0.3183 - acc: 0.823 - ETA: 0s - loss: 0.3179 - acc: 0.821 - ETA: 0s - loss: 0.3233 - acc: 0.818 - 0s 333us/step - loss: 0.3192 - acc: 0.8240 - val_loss: 3.1493 - val_acc: 0.2283\n",
      "Epoch 24/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2946 - acc: 0.812 - ETA: 0s - loss: 0.2498 - acc: 0.857 - ETA: 0s - loss: 0.2636 - acc: 0.859 - ETA: 0s - loss: 0.2778 - acc: 0.853 - ETA: 0s - loss: 0.2915 - acc: 0.833 - ETA: 0s - loss: 0.3104 - acc: 0.820 - ETA: 0s - loss: 0.3079 - acc: 0.823 - ETA: 0s - loss: 0.3087 - acc: 0.823 - 0s 360us/step - loss: 0.3104 - acc: 0.8240 - val_loss: 2.3136 - val_acc: 0.3248\n",
      "Epoch 25/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3803 - acc: 0.718 - ETA: 0s - loss: 0.2922 - acc: 0.843 - ETA: 0s - loss: 0.2950 - acc: 0.843 - ETA: 0s - loss: 0.2923 - acc: 0.843 - ETA: 0s - loss: 0.3003 - acc: 0.840 - ETA: 0s - loss: 0.2937 - acc: 0.843 - ETA: 0s - loss: 0.3037 - acc: 0.835 - ETA: 0s - loss: 0.3098 - acc: 0.830 - 0s 365us/step - loss: 0.3189 - acc: 0.8256 - val_loss: 2.4965 - val_acc: 0.2830\n",
      "Epoch 26/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3119 - acc: 0.937 - ETA: 0s - loss: 0.3248 - acc: 0.821 - ETA: 0s - loss: 0.3080 - acc: 0.820 - ETA: 0s - loss: 0.3132 - acc: 0.828 - ETA: 0s - loss: 0.3123 - acc: 0.832 - ETA: 0s - loss: 0.3026 - acc: 0.837 - ETA: 0s - loss: 0.3034 - acc: 0.837 - ETA: 0s - loss: 0.3048 - acc: 0.838 - 0s 362us/step - loss: 0.3110 - acc: 0.8328 - val_loss: 2.6675 - val_acc: 0.2637\n",
      "Epoch 27/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3253 - acc: 0.812 - ETA: 0s - loss: 0.2835 - acc: 0.843 - ETA: 0s - loss: 0.2939 - acc: 0.834 - ETA: 0s - loss: 0.2945 - acc: 0.834 - ETA: 0s - loss: 0.2978 - acc: 0.838 - ETA: 0s - loss: 0.3010 - acc: 0.833 - ETA: 0s - loss: 0.3067 - acc: 0.825 - ETA: 0s - loss: 0.3014 - acc: 0.828 - 0s 379us/step - loss: 0.3062 - acc: 0.8240 - val_loss: 2.1504 - val_acc: 0.3151\n",
      "Epoch 28/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2731 - acc: 0.875 - ETA: 0s - loss: 0.3379 - acc: 0.799 - ETA: 0s - loss: 0.3071 - acc: 0.838 - ETA: 0s - loss: 0.3088 - acc: 0.836 - ETA: 0s - loss: 0.3147 - acc: 0.839 - ETA: 0s - loss: 0.3099 - acc: 0.842 - ETA: 0s - loss: 0.3046 - acc: 0.842 - ETA: 0s - loss: 0.3098 - acc: 0.838 - 0s 350us/step - loss: 0.3078 - acc: 0.8408 - val_loss: 2.8693 - val_acc: 0.2733\n",
      "Epoch 29/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2581 - acc: 0.843 - ETA: 0s - loss: 0.2783 - acc: 0.857 - ETA: 0s - loss: 0.2893 - acc: 0.835 - ETA: 0s - loss: 0.2922 - acc: 0.836 - ETA: 0s - loss: 0.3007 - acc: 0.829 - ETA: 0s - loss: 0.2970 - acc: 0.835 - ETA: 0s - loss: 0.3026 - acc: 0.831 - ETA: 0s - loss: 0.3013 - acc: 0.831 - 0s 359us/step - loss: 0.3069 - acc: 0.8304 - val_loss: 1.6122 - val_acc: 0.4277\n",
      "Epoch 30/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3481 - acc: 0.812 - ETA: 0s - loss: 0.3331 - acc: 0.820 - ETA: 0s - loss: 0.3261 - acc: 0.812 - ETA: 0s - loss: 0.3120 - acc: 0.822 - ETA: 0s - loss: 0.2842 - acc: 0.849 - ETA: 0s - loss: 0.3035 - acc: 0.840 - ETA: 0s - loss: 0.3131 - acc: 0.834 - ETA: 0s - loss: 0.3023 - acc: 0.842 - ETA: 0s - loss: 0.3050 - acc: 0.842 - 1s 412us/step - loss: 0.3084 - acc: 0.8376 - val_loss: 1.7320 - val_acc: 0.4598\n",
      "Epoch 31/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2927 - acc: 0.812 - ETA: 0s - loss: 0.3104 - acc: 0.856 - ETA: 0s - loss: 0.3102 - acc: 0.843 - ETA: 0s - loss: 0.2952 - acc: 0.843 - ETA: 0s - loss: 0.3005 - acc: 0.839 - ETA: 0s - loss: 0.3056 - acc: 0.828 - ETA: 0s - loss: 0.3010 - acc: 0.828 - ETA: 0s - loss: 0.2961 - acc: 0.833 - 0s 381us/step - loss: 0.3007 - acc: 0.8320 - val_loss: 1.4474 - val_acc: 0.4534\n",
      "Epoch 32/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3180 - acc: 0.875 - ETA: 0s - loss: 0.3031 - acc: 0.812 - ETA: 0s - loss: 0.2986 - acc: 0.820 - ETA: 0s - loss: 0.2930 - acc: 0.830 - ETA: 0s - loss: 0.2831 - acc: 0.839 - ETA: 0s - loss: 0.2827 - acc: 0.838 - ETA: 0s - loss: 0.2840 - acc: 0.839 - ETA: 0s - loss: 0.2875 - acc: 0.838 - ETA: 0s - loss: 0.2932 - acc: 0.838 - 1s 421us/step - loss: 0.2968 - acc: 0.8376 - val_loss: 3.3838 - val_acc: 0.2219\n",
      "Epoch 33/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3766 - acc: 0.812 - ETA: 0s - loss: 0.3055 - acc: 0.822 - ETA: 0s - loss: 0.2899 - acc: 0.832 - ETA: 0s - loss: 0.2949 - acc: 0.828 - ETA: 0s - loss: 0.3029 - acc: 0.830 - ETA: 0s - loss: 0.3008 - acc: 0.833 - ETA: 0s - loss: 0.3014 - acc: 0.836 - ETA: 0s - loss: 0.2948 - acc: 0.837 - ETA: 0s - loss: 0.2947 - acc: 0.838 - 0s 393us/step - loss: 0.2952 - acc: 0.8368 - val_loss: 1.8819 - val_acc: 0.4823\n",
      "Epoch 34/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2086 - acc: 0.968 - ETA: 0s - loss: 0.2833 - acc: 0.849 - ETA: 0s - loss: 0.2821 - acc: 0.849 - ETA: 0s - loss: 0.2712 - acc: 0.851 - ETA: 0s - loss: 0.2723 - acc: 0.848 - ETA: 0s - loss: 0.2796 - acc: 0.847 - ETA: 0s - loss: 0.2747 - acc: 0.848 - ETA: 0s - loss: 0.2876 - acc: 0.840 - 0s 349us/step - loss: 0.2942 - acc: 0.8328 - val_loss: 2.3205 - val_acc: 0.3505\n",
      "Epoch 35/50\n",
      "1244/1244 [==============================] - ETA: 1s - loss: 0.3806 - acc: 0.781 - ETA: 0s - loss: 0.2643 - acc: 0.870 - ETA: 0s - loss: 0.2593 - acc: 0.855 - ETA: 0s - loss: 0.2705 - acc: 0.849 - ETA: 0s - loss: 0.2762 - acc: 0.846 - ETA: 0s - loss: 0.2905 - acc: 0.836 - ETA: 0s - loss: 0.2943 - acc: 0.833 - ETA: 0s - loss: 0.2914 - acc: 0.829 - 0s 397us/step - loss: 0.2934 - acc: 0.8280 - val_loss: 2.7741 - val_acc: 0.3633\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244/1244 [==============================] - ETA: 0s - loss: 0.1884 - acc: 0.906 - ETA: 0s - loss: 0.2759 - acc: 0.833 - ETA: 0s - loss: 0.2935 - acc: 0.822 - ETA: 0s - loss: 0.2800 - acc: 0.828 - ETA: 0s - loss: 0.2879 - acc: 0.831 - ETA: 0s - loss: 0.2837 - acc: 0.835 - ETA: 0s - loss: 0.2883 - acc: 0.835 - ETA: 0s - loss: 0.2902 - acc: 0.832 - 0s 377us/step - loss: 0.2935 - acc: 0.8336 - val_loss: 2.2678 - val_acc: 0.3119\n",
      "Epoch 37/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2731 - acc: 0.843 - ETA: 0s - loss: 0.2550 - acc: 0.843 - ETA: 0s - loss: 0.2249 - acc: 0.868 - ETA: 0s - loss: 0.2551 - acc: 0.854 - ETA: 0s - loss: 0.2725 - acc: 0.843 - ETA: 0s - loss: 0.2752 - acc: 0.843 - ETA: 0s - loss: 0.2788 - acc: 0.842 - ETA: 0s - loss: 0.2855 - acc: 0.841 - ETA: 0s - loss: 0.2865 - acc: 0.838 - 0s 400us/step - loss: 0.2907 - acc: 0.8368 - val_loss: 2.8663 - val_acc: 0.3601\n",
      "Epoch 38/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2822 - acc: 0.843 - ETA: 0s - loss: 0.3018 - acc: 0.833 - ETA: 0s - loss: 0.2995 - acc: 0.832 - ETA: 0s - loss: 0.2864 - acc: 0.837 - ETA: 0s - loss: 0.3003 - acc: 0.836 - ETA: 0s - loss: 0.3039 - acc: 0.830 - ETA: 0s - loss: 0.2983 - acc: 0.836 - ETA: 0s - loss: 0.2977 - acc: 0.835 - 0s 354us/step - loss: 0.2956 - acc: 0.8328 - val_loss: 2.5986 - val_acc: 0.3441\n",
      "Epoch 39/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2283 - acc: 0.875 - ETA: 0s - loss: 0.2763 - acc: 0.838 - ETA: 0s - loss: 0.3095 - acc: 0.821 - ETA: 0s - loss: 0.3087 - acc: 0.816 - ETA: 0s - loss: 0.3092 - acc: 0.820 - ETA: 0s - loss: 0.3081 - acc: 0.817 - ETA: 0s - loss: 0.2985 - acc: 0.825 - ETA: 0s - loss: 0.2954 - acc: 0.829 - ETA: 0s - loss: 0.2911 - acc: 0.831 - 1s 422us/step - loss: 0.2856 - acc: 0.8328 - val_loss: 3.3193 - val_acc: 0.3762\n",
      "Epoch 40/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3808 - acc: 0.750 - ETA: 0s - loss: 0.3291 - acc: 0.796 - ETA: 1s - loss: 0.3017 - acc: 0.831 - ETA: 1s - loss: 0.2949 - acc: 0.833 - ETA: 0s - loss: 0.3018 - acc: 0.832 - ETA: 0s - loss: 0.2848 - acc: 0.846 - ETA: 0s - loss: 0.2744 - acc: 0.854 - ETA: 0s - loss: 0.2798 - acc: 0.848 - ETA: 0s - loss: 0.2876 - acc: 0.846 - ETA: 0s - loss: 0.2874 - acc: 0.846 - ETA: 0s - loss: 0.2840 - acc: 0.840 - ETA: 0s - loss: 0.2873 - acc: 0.837 - 1s 577us/step - loss: 0.2908 - acc: 0.8384 - val_loss: 2.9676 - val_acc: 0.3280\n",
      "Epoch 41/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2418 - acc: 0.812 - ETA: 0s - loss: 0.2328 - acc: 0.850 - ETA: 0s - loss: 0.2417 - acc: 0.843 - ETA: 0s - loss: 0.2767 - acc: 0.825 - ETA: 0s - loss: 0.2692 - acc: 0.831 - ETA: 0s - loss: 0.2715 - acc: 0.842 - ETA: 0s - loss: 0.2742 - acc: 0.840 - ETA: 0s - loss: 0.2767 - acc: 0.841 - ETA: 0s - loss: 0.2790 - acc: 0.839 - ETA: 0s - loss: 0.2884 - acc: 0.828 - ETA: 0s - loss: 0.2838 - acc: 0.831 - 1s 511us/step - loss: 0.2864 - acc: 0.8336 - val_loss: 2.8314 - val_acc: 0.3923\n",
      "Epoch 42/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2043 - acc: 0.906 - ETA: 0s - loss: 0.2638 - acc: 0.843 - ETA: 0s - loss: 0.2683 - acc: 0.835 - ETA: 0s - loss: 0.2868 - acc: 0.835 - ETA: 0s - loss: 0.2783 - acc: 0.843 - ETA: 0s - loss: 0.2797 - acc: 0.836 - ETA: 0s - loss: 0.2813 - acc: 0.836 - ETA: 0s - loss: 0.2737 - acc: 0.843 - ETA: 0s - loss: 0.2700 - acc: 0.849 - ETA: 0s - loss: 0.2819 - acc: 0.840 - ETA: 0s - loss: 0.2824 - acc: 0.839 - 1s 478us/step - loss: 0.2854 - acc: 0.8360 - val_loss: 2.6052 - val_acc: 0.3569\n",
      "Epoch 43/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3215 - acc: 0.750 - ETA: 0s - loss: 0.2727 - acc: 0.864 - ETA: 0s - loss: 0.2483 - acc: 0.880 - ETA: 0s - loss: 0.2604 - acc: 0.868 - ETA: 0s - loss: 0.2745 - acc: 0.853 - ETA: 0s - loss: 0.2709 - acc: 0.854 - ETA: 0s - loss: 0.2791 - acc: 0.848 - ETA: 0s - loss: 0.2783 - acc: 0.852 - ETA: 0s - loss: 0.2882 - acc: 0.843 - 0s 390us/step - loss: 0.2894 - acc: 0.8465 - val_loss: 3.4683 - val_acc: 0.2669\n",
      "Epoch 44/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2486 - acc: 0.906 - ETA: 0s - loss: 0.2910 - acc: 0.828 - ETA: 0s - loss: 0.2743 - acc: 0.840 - ETA: 0s - loss: 0.2562 - acc: 0.850 - ETA: 0s - loss: 0.2641 - acc: 0.840 - ETA: 0s - loss: 0.2695 - acc: 0.835 - ETA: 0s - loss: 0.2764 - acc: 0.835 - ETA: 0s - loss: 0.2800 - acc: 0.830 - ETA: 0s - loss: 0.2848 - acc: 0.832 - 1s 448us/step - loss: 0.2817 - acc: 0.8336 - val_loss: 3.2307 - val_acc: 0.3151\n",
      "Epoch 45/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2099 - acc: 0.906 - ETA: 0s - loss: 0.2807 - acc: 0.864 - ETA: 0s - loss: 0.3004 - acc: 0.825 - ETA: 0s - loss: 0.2795 - acc: 0.839 - ETA: 0s - loss: 0.2807 - acc: 0.833 - ETA: 0s - loss: 0.2777 - acc: 0.832 - ETA: 0s - loss: 0.2882 - acc: 0.825 - ETA: 0s - loss: 0.2859 - acc: 0.833 - ETA: 0s - loss: 0.2848 - acc: 0.833 - 1s 412us/step - loss: 0.2836 - acc: 0.8344 - val_loss: 2.5559 - val_acc: 0.3891\n",
      "Epoch 46/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2311 - acc: 0.906 - ETA: 0s - loss: 0.2652 - acc: 0.890 - ETA: 0s - loss: 0.2684 - acc: 0.881 - ETA: 0s - loss: 0.2769 - acc: 0.868 - ETA: 0s - loss: 0.2649 - acc: 0.868 - ETA: 0s - loss: 0.2758 - acc: 0.856 - ETA: 0s - loss: 0.2812 - acc: 0.849 - ETA: 0s - loss: 0.2869 - acc: 0.843 - ETA: 0s - loss: 0.2836 - acc: 0.843 - 1s 405us/step - loss: 0.2829 - acc: 0.8432 - val_loss: 4.3819 - val_acc: 0.2765\n",
      "Epoch 47/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.2480 - acc: 0.843 - ETA: 0s - loss: 0.2749 - acc: 0.833 - ETA: 0s - loss: 0.2585 - acc: 0.858 - ETA: 0s - loss: 0.2717 - acc: 0.845 - ETA: 0s - loss: 0.2688 - acc: 0.846 - ETA: 0s - loss: 0.2700 - acc: 0.856 - ETA: 0s - loss: 0.2740 - acc: 0.848 - ETA: 0s - loss: 0.2724 - acc: 0.844 - ETA: 0s - loss: 0.2811 - acc: 0.832 - 1s 404us/step - loss: 0.2819 - acc: 0.8320 - val_loss: 3.1969 - val_acc: 0.3183\n",
      "Epoch 48/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3489 - acc: 0.812 - ETA: 0s - loss: 0.2822 - acc: 0.834 - ETA: 0s - loss: 0.2685 - acc: 0.843 - ETA: 0s - loss: 0.2637 - acc: 0.849 - ETA: 0s - loss: 0.2680 - acc: 0.849 - ETA: 0s - loss: 0.2983 - acc: 0.841 - ETA: 0s - loss: 0.2947 - acc: 0.840 - ETA: 0s - loss: 0.2968 - acc: 0.835 - 0s 383us/step - loss: 0.2938 - acc: 0.8352 - val_loss: 2.9599 - val_acc: 0.3473\n",
      "Epoch 49/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3061 - acc: 0.781 - ETA: 0s - loss: 0.2259 - acc: 0.869 - ETA: 0s - loss: 0.2243 - acc: 0.885 - ETA: 0s - loss: 0.2302 - acc: 0.873 - ETA: 0s - loss: 0.2485 - acc: 0.864 - ETA: 0s - loss: 0.2702 - acc: 0.843 - ETA: 0s - loss: 0.2710 - acc: 0.843 - ETA: 0s - loss: 0.2779 - acc: 0.838 - 0s 376us/step - loss: 0.2784 - acc: 0.8400 - val_loss: 3.7665 - val_acc: 0.3087\n",
      "Epoch 50/50\n",
      "1244/1244 [==============================] - ETA: 0s - loss: 0.3370 - acc: 0.781 - ETA: 0s - loss: 0.2618 - acc: 0.849 - ETA: 0s - loss: 0.2683 - acc: 0.838 - ETA: 0s - loss: 0.2692 - acc: 0.847 - ETA: 0s - loss: 0.2792 - acc: 0.834 - ETA: 0s - loss: 0.2770 - acc: 0.839 - ETA: 0s - loss: 0.2727 - acc: 0.844 - 0s 344us/step - loss: 0.2813 - acc: 0.8424 - val_loss: 1.8431 - val_acc: 0.4309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c2026d710>"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(bert_embed,y,batch_size=32,epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
